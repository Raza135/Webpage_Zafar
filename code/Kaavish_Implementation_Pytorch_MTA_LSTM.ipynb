{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kaavish_Implementation_Pytorch_MTA-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzq_be34AGVj"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MTA-LSTM-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDqgCjAgzx7v"
      },
      "source": [
        "# ! pip install matplotlib==3.1.1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJmTb-GteOqF",
        "outputId": "aaed543b-1fd4-48fb-f011-80d9bb50cb83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlPCWPYmAKrU"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, autograd, optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn import Parameter, LayerNorm\n",
        "from torch.autograd import Variable\n",
        "import torch.jit as jit\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ui4ZK3f58Nb"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMADkKY2fObl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ee47ab-4ba7-43df-f71b-086dabb4e5f1"
      },
      "source": [
        "print(gensim.__version__)\n",
        "print(torch.__version__)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.0\n",
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGAVjaCAAP9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4431ffd2-2eb7-4887-b9e3-de9d1769f037"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "print('Available cuda:', torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    device_num = 0\n",
        "    deviceName = \"cuda:%d\" % device_num\n",
        "    torch.cuda.set_device(device_num)\n",
        "    print('Current device:', torch.cuda.current_device())\n",
        "else:\n",
        "    deviceName = \"cpu\"\n",
        "    \n",
        "device = torch.device(deviceName)\n",
        "print(deviceName)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available cuda: 1\n",
            "Current device: 0\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FruHIU-AcBM"
      },
      "source": [
        "\n",
        "**Build a dictionary and pretrained embedding system**\n",
        "\n",
        "Here I'm gonna load the pretrained word2vec vocab and vectors. Please refer to this notebook to he how to train it.\n",
        "\n",
        "The code fvec.vectors is where we get the pretrained vectors. <PAD>, <BOS>, <EOS> and <UNK> are 4 common tokens which stands for PADding, Begin-Of-Sentence, End-Of-Sentence and UNKnown respectively. We simply add them into the vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdMcDI1pAQAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9aa28e-7ab0-4eac-81a4-9b656e212dc0"
      },
      "source": [
        " try:\n",
        "   file_path = '/content/drive/MyDrive/Colab Notebooks/Kaavish/Kaavish_Implementation_01_res/UrduW2V/data_big_W2V.txt'\n",
        " finally:\n",
        "   file_path = '/content/drive/MyDrive/Kaavish_Implementation_01_res/UrduW2V/data_big_W2V.txt'\n",
        "\n",
        "#file_path = \"/content/drive/MyDrive/Colab Notebooks/Kaavish/Kaavish_Implementation_01_res/UrduW2V/data_big_W2V.txt\"\n",
        "\n",
        "fvec = KeyedVectors.load_word2vec_format(file_path, binary=False)\n",
        "word_vec = fvec.vectors\n",
        "vocab = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
        "vocab.extend(list(fvec.vocab.keys()))\n",
        "word_vec = np.concatenate((np.array([[0]*word_vec.shape[1]] * 4), word_vec))\n",
        "word_vec = torch.tensor(word_vec).float()\n",
        "del fvec\n",
        "print(\"total %d words\" % len(word_vec))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 146817 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIN8BH_j1Z2L"
      },
      "source": [
        "try:\n",
        "  save_folder = '/content/drive/MyDrive/Colab Notebooks/Kaavish/Kaavish_Implementation_01_res/modelTrained'\n",
        "finally:\n",
        "  save_folder = '/content/drive/MyDrive/Kaavish_Implementation_01_res/modelTrained'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdscwHcAAQDP"
      },
      "source": [
        "#save_folder = '/content/drive/MyDrive/Colab Notebooks/Kaavish/Kaavish_Implementation_01_res/modelTrained'\n",
        "vocab_check_point = '%s/vocab.pkl' % save_folder\n",
        "word_vec_check_point = '%s/word_vec.pkl' % save_folder\n",
        "torch.save(vocab, vocab_check_point)\n",
        "torch.save(word_vec, word_vec_check_point)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfQ9vYoYmEFg"
      },
      "source": [
        "\n",
        "**Build a word-index convertor**\n",
        "\n",
        "We don't want to use type of string directly when training, instead we map them to a unique index in integer. In text generation phase, we'll then convert them back to string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYB--Y7gAQF3"
      },
      "source": [
        "word_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "idx_to_word = {i: ch for i, ch in enumerate(vocab)}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0_YsbTgmMm4"
      },
      "source": [
        "**Load preprocessed data**\n",
        "\n",
        "You can prepare for your own data, or simply use what I offered in the data folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW60MmLjmPej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace60ae5-23d1-46a3-a383-8709727960fa"
      },
      "source": [
        "essays = []\n",
        "topics = []\n",
        "\n",
        "try:\n",
        "  data_path = \"/content/drive/MyDrive/Colab Notebooks/Kaavish/Kaavish_Implementation_01_res/data_keyed_5_shortPara.txt\"\n",
        "finally:\n",
        "  data_path = \"/content/drive/MyDrive/Kaavish_Implementation_01_res/data_keyed_5_shortPara.txt\"\n",
        "\n",
        "num_lines = sum(1 for line in open(data_path, 'r'))\n",
        "\n",
        "with open(data_path) as f:\n",
        "    for line in tqdm(f, total=num_lines):\n",
        "        essay, topic = line.replace('\\n', '').split(' </d> ')\n",
        "        # print(essay)\n",
        "        # print(topic)\n",
        "        essays.append(essay.strip().split(' '))\n",
        "        topics.append(topic.strip().split(' '))\n",
        "    f.close()\n",
        "\n",
        "assert len(topics) == len(essays)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 92325/92325 [00:02<00:00, 36992.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Teli60sd63Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481c708b-e8e9-4f4b-8beb-ecd6c1f8299a"
      },
      "source": [
        "print(len(essays))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqmE-HJvmUdN"
      },
      "source": [
        "We then map all the training and testing corpus to integer index word-by-word, with the help of our convertor. Note that we map it to <UNK> if the words in corpus are not in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oEqG2VOmSyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff74ed9-cddc-4ae9-8cda-0f9197f401de"
      },
      "source": [
        "divider = 90000        # no. at which you want to divide corpus into train and test samples\n",
        "total = 92325          # total no. of samples -- replace with len(essays) if want to use all samples in dataset\n",
        "\n",
        "corpus_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[:divider])))\n",
        "topics_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[:divider])))\n",
        "corpus_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[divider:total])))\n",
        "topics_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[divider:total])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 90000/90000 [00:02<00:00, 31843.27it/s]\n",
            "100%|██████████| 90000/90000 [00:00<00:00, 341846.45it/s]\n",
            "100%|██████████| 2325/2325 [00:00<00:00, 34980.87it/s]\n",
            "100%|██████████| 2325/2325 [00:00<00:00, 292415.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQVWQoMuMrJ-",
        "outputId": "713be2f9-6878-437b-b7cb-e41bd9121ab3"
      },
      "source": [
        "print(len(corpus_test))\n",
        "print(len(essays))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2325\n",
            "92325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4dXXRPWmX3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d50c928-6982-4800-b834-0015bf34a3c0"
      },
      "source": [
        "\n",
        "def viewData(topics, X):\n",
        "    topics = [idx_to_word[x] for x in topics]\n",
        "    X = [idx_to_word[x] for x in X]\n",
        "    #print(topics, X)\n",
        "    print(topics)\n",
        "\n",
        "#print(len(corpus_indice))\n",
        "\n",
        "# viewData(topics_indice[10], corpus_indice[10])\n",
        "\n",
        "for i in range(1000,1010):\n",
        "    viewData(topics_indice[i], corpus_indice[i])\n",
        "    print(len(topics_indice[i]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['مدد', 'حوصلہ', 'فراہم', 'بھیجے', 'وسائل']\n",
            "5\n",
            "['بھرپور', 'کوشش', 'قوت', 'امریکہ', 'امریکا']\n",
            "5\n",
            "['ہو', 'کردیا', 'زخمی', 'درجنوں', 'شدید']\n",
            "5\n",
            "['فورسز', 'جس', 'قبضہ', 'عراق', 'افواج']\n",
            "5\n",
            "['قبضہ', 'فلوجہ', 'ہوگئے', 'رمادی', 'تقریبا']\n",
            "5\n",
            "['وکٹ', 'تیسرا', 'انڈیز', 'دے', 'نیوزی']\n",
            "5\n",
            "['جواب', 'کن', 'ساتھ', 'سیریز', '3-0']\n",
            "5\n",
            "['گئے', 'روزہ', 'ہدف', 'باؤلنگ', 'عوض']\n",
            "5\n",
            "['ہو', 'سٹیفنی', 'جواب', 'نمایاں', 'مطلوبہ']\n",
            "5\n",
            "['اسے', 'جائے', 'رہی', 'میچ', 'وڈ']\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDIiDs81AQIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2148629-cc86-4190-b77d-222078725282"
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "def shuffleData(topics_indice, corpus_indice):\n",
        "    ind_list = [i for i in range(len(topics_indice))]\n",
        "    shuffle(ind_list)\n",
        "    topics_indice = np.array(topics_indice)\n",
        "    corpus_indice = np.array(corpus_indice)\n",
        "    topics_indice = topics_indice[ind_list,]\n",
        "    corpus_indice = corpus_indice[ind_list,]\n",
        "    topics_indice = topics_indice.tolist()\n",
        "    corpus_indice = corpus_indice.tolist()\n",
        "    return topics_indice, corpus_indice\n",
        "\n",
        "# topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice)\n",
        "viewData(topics_indice[0], corpus_indice[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['کھیلے', 'زمبابوے', 'آخری', 'دیش', 'گئے']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvq3HBGLmcKW"
      },
      "source": [
        "for t in topics_indice:\n",
        "    if len(t) != 5:\n",
        "        print('less than 5')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaRPIHbKAQNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b9f84d-8bce-4a20-a046-41901abb8dd7"
      },
      "source": [
        "length = list(map(lambda x: len(x), corpus_indice))\n",
        "length"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[98,\n",
              " 90,\n",
              " 80,\n",
              " 86,\n",
              " 88,\n",
              " 99,\n",
              " 87,\n",
              " 82,\n",
              " 86,\n",
              " 96,\n",
              " 89,\n",
              " 97,\n",
              " 80,\n",
              " 97,\n",
              " 82,\n",
              " 83,\n",
              " 87,\n",
              " 80,\n",
              " 94,\n",
              " 87,\n",
              " 91,\n",
              " 98,\n",
              " 91,\n",
              " 91,\n",
              " 94,\n",
              " 91,\n",
              " 92,\n",
              " 87,\n",
              " 97,\n",
              " 96,\n",
              " 93,\n",
              " 85,\n",
              " 85,\n",
              " 93,\n",
              " 82,\n",
              " 94,\n",
              " 81,\n",
              " 84,\n",
              " 89,\n",
              " 85,\n",
              " 99,\n",
              " 94,\n",
              " 81,\n",
              " 92,\n",
              " 91,\n",
              " 91,\n",
              " 93,\n",
              " 94,\n",
              " 89,\n",
              " 98,\n",
              " 99,\n",
              " 90,\n",
              " 83,\n",
              " 82,\n",
              " 97,\n",
              " 95,\n",
              " 97,\n",
              " 89,\n",
              " 88,\n",
              " 83,\n",
              " 89,\n",
              " 83,\n",
              " 99,\n",
              " 96,\n",
              " 90,\n",
              " 90,\n",
              " 88,\n",
              " 83,\n",
              " 82,\n",
              " 97,\n",
              " 94,\n",
              " 98,\n",
              " 98,\n",
              " 92,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 85,\n",
              " 88,\n",
              " 91,\n",
              " 93,\n",
              " 99,\n",
              " 91,\n",
              " 97,\n",
              " 92,\n",
              " 84,\n",
              " 84,\n",
              " 90,\n",
              " 85,\n",
              " 90,\n",
              " 89,\n",
              " 91,\n",
              " 84,\n",
              " 89,\n",
              " 95,\n",
              " 100,\n",
              " 84,\n",
              " 87,\n",
              " 90,\n",
              " 90,\n",
              " 92,\n",
              " 82,\n",
              " 93,\n",
              " 80,\n",
              " 99,\n",
              " 92,\n",
              " 98,\n",
              " 92,\n",
              " 83,\n",
              " 85,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 90,\n",
              " 93,\n",
              " 97,\n",
              " 90,\n",
              " 97,\n",
              " 100,\n",
              " 87,\n",
              " 98,\n",
              " 95,\n",
              " 97,\n",
              " 83,\n",
              " 87,\n",
              " 88,\n",
              " 86,\n",
              " 88,\n",
              " 95,\n",
              " 95,\n",
              " 80,\n",
              " 90,\n",
              " 89,\n",
              " 96,\n",
              " 83,\n",
              " 89,\n",
              " 89,\n",
              " 100,\n",
              " 100,\n",
              " 99,\n",
              " 92,\n",
              " 81,\n",
              " 87,\n",
              " 96,\n",
              " 81,\n",
              " 100,\n",
              " 82,\n",
              " 85,\n",
              " 85,\n",
              " 83,\n",
              " 84,\n",
              " 90,\n",
              " 94,\n",
              " 81,\n",
              " 80,\n",
              " 80,\n",
              " 81,\n",
              " 84,\n",
              " 98,\n",
              " 89,\n",
              " 88,\n",
              " 94,\n",
              " 83,\n",
              " 85,\n",
              " 96,\n",
              " 91,\n",
              " 99,\n",
              " 82,\n",
              " 97,\n",
              " 94,\n",
              " 97,\n",
              " 86,\n",
              " 98,\n",
              " 90,\n",
              " 84,\n",
              " 94,\n",
              " 94,\n",
              " 91,\n",
              " 89,\n",
              " 85,\n",
              " 97,\n",
              " 92,\n",
              " 98,\n",
              " 81,\n",
              " 80,\n",
              " 100,\n",
              " 91,\n",
              " 94,\n",
              " 84,\n",
              " 95,\n",
              " 88,\n",
              " 80,\n",
              " 92,\n",
              " 83,\n",
              " 92,\n",
              " 81,\n",
              " 90,\n",
              " 83,\n",
              " 91,\n",
              " 80,\n",
              " 98,\n",
              " 83,\n",
              " 98,\n",
              " 93,\n",
              " 88,\n",
              " 86,\n",
              " 95,\n",
              " 92,\n",
              " 100,\n",
              " 96,\n",
              " 84,\n",
              " 91,\n",
              " 82,\n",
              " 100,\n",
              " 84,\n",
              " 84,\n",
              " 98,\n",
              " 90,\n",
              " 81,\n",
              " 100,\n",
              " 82,\n",
              " 100,\n",
              " 89,\n",
              " 85,\n",
              " 87,\n",
              " 80,\n",
              " 95,\n",
              " 98,\n",
              " 88,\n",
              " 99,\n",
              " 96,\n",
              " 94,\n",
              " 86,\n",
              " 85,\n",
              " 85,\n",
              " 90,\n",
              " 92,\n",
              " 94,\n",
              " 90,\n",
              " 95,\n",
              " 100,\n",
              " 98,\n",
              " 82,\n",
              " 87,\n",
              " 80,\n",
              " 93,\n",
              " 99,\n",
              " 96,\n",
              " 81,\n",
              " 93,\n",
              " 87,\n",
              " 80,\n",
              " 86,\n",
              " 85,\n",
              " 95,\n",
              " 94,\n",
              " 84,\n",
              " 80,\n",
              " 98,\n",
              " 97,\n",
              " 96,\n",
              " 86,\n",
              " 97,\n",
              " 88,\n",
              " 93,\n",
              " 83,\n",
              " 81,\n",
              " 80,\n",
              " 90,\n",
              " 83,\n",
              " 100,\n",
              " 85,\n",
              " 86,\n",
              " 94,\n",
              " 90,\n",
              " 94,\n",
              " 86,\n",
              " 83,\n",
              " 90,\n",
              " 91,\n",
              " 90,\n",
              " 90,\n",
              " 85,\n",
              " 82,\n",
              " 100,\n",
              " 95,\n",
              " 98,\n",
              " 95,\n",
              " 100,\n",
              " 97,\n",
              " 83,\n",
              " 98,\n",
              " 93,\n",
              " 96,\n",
              " 88,\n",
              " 82,\n",
              " 99,\n",
              " 86,\n",
              " 83,\n",
              " 90,\n",
              " 91,\n",
              " 89,\n",
              " 90,\n",
              " 84,\n",
              " 97,\n",
              " 82,\n",
              " 91,\n",
              " 86,\n",
              " 89,\n",
              " 95,\n",
              " 98,\n",
              " 97,\n",
              " 99,\n",
              " 92,\n",
              " 95,\n",
              " 82,\n",
              " 89,\n",
              " 87,\n",
              " 87,\n",
              " 80,\n",
              " 91,\n",
              " 93,\n",
              " 100,\n",
              " 82,\n",
              " 99,\n",
              " 86,\n",
              " 83,\n",
              " 92,\n",
              " 88,\n",
              " 92,\n",
              " 95,\n",
              " 91,\n",
              " 87,\n",
              " 94,\n",
              " 93,\n",
              " 88,\n",
              " 91,\n",
              " 98,\n",
              " 98,\n",
              " 98,\n",
              " 82,\n",
              " 93,\n",
              " 87,\n",
              " 95,\n",
              " 89,\n",
              " 81,\n",
              " 87,\n",
              " 80,\n",
              " 99,\n",
              " 80,\n",
              " 96,\n",
              " 89,\n",
              " 87,\n",
              " 91,\n",
              " 90,\n",
              " 91,\n",
              " 97,\n",
              " 97,\n",
              " 91,\n",
              " 97,\n",
              " 85,\n",
              " 82,\n",
              " 81,\n",
              " 80,\n",
              " 87,\n",
              " 93,\n",
              " 92,\n",
              " 97,\n",
              " 97,\n",
              " 99,\n",
              " 93,\n",
              " 86,\n",
              " 82,\n",
              " 83,\n",
              " 85,\n",
              " 95,\n",
              " 82,\n",
              " 80,\n",
              " 97,\n",
              " 88,\n",
              " 80,\n",
              " 87,\n",
              " 94,\n",
              " 97,\n",
              " 81,\n",
              " 87,\n",
              " 88,\n",
              " 93,\n",
              " 83,\n",
              " 86,\n",
              " 83,\n",
              " 82,\n",
              " 91,\n",
              " 100,\n",
              " 96,\n",
              " 95,\n",
              " 91,\n",
              " 93,\n",
              " 95,\n",
              " 88,\n",
              " 84,\n",
              " 87,\n",
              " 92,\n",
              " 85,\n",
              " 85,\n",
              " 88,\n",
              " 92,\n",
              " 92,\n",
              " 86,\n",
              " 95,\n",
              " 82,\n",
              " 90,\n",
              " 92,\n",
              " 92,\n",
              " 91,\n",
              " 92,\n",
              " 82,\n",
              " 87,\n",
              " 81,\n",
              " 85,\n",
              " 88,\n",
              " 88,\n",
              " 82,\n",
              " 80,\n",
              " 90,\n",
              " 85,\n",
              " 80,\n",
              " 83,\n",
              " 96,\n",
              " 88,\n",
              " 89,\n",
              " 85,\n",
              " 81,\n",
              " 98,\n",
              " 89,\n",
              " 91,\n",
              " 91,\n",
              " 93,\n",
              " 86,\n",
              " 93,\n",
              " 96,\n",
              " 95,\n",
              " 98,\n",
              " 87,\n",
              " 92,\n",
              " 80,\n",
              " 86,\n",
              " 86,\n",
              " 100,\n",
              " 80,\n",
              " 85,\n",
              " 84,\n",
              " 95,\n",
              " 87,\n",
              " 89,\n",
              " 95,\n",
              " 92,\n",
              " 92,\n",
              " 88,\n",
              " 97,\n",
              " 96,\n",
              " 83,\n",
              " 95,\n",
              " 96,\n",
              " 86,\n",
              " 82,\n",
              " 96,\n",
              " 85,\n",
              " 95,\n",
              " 88,\n",
              " 85,\n",
              " 80,\n",
              " 95,\n",
              " 90,\n",
              " 95,\n",
              " 97,\n",
              " 84,\n",
              " 91,\n",
              " 90,\n",
              " 80,\n",
              " 97,\n",
              " 96,\n",
              " 98,\n",
              " 81,\n",
              " 98,\n",
              " 81,\n",
              " 95,\n",
              " 89,\n",
              " 96,\n",
              " 86,\n",
              " 87,\n",
              " 89,\n",
              " 100,\n",
              " 90,\n",
              " 92,\n",
              " 83,\n",
              " 80,\n",
              " 90,\n",
              " 98,\n",
              " 98,\n",
              " 87,\n",
              " 81,\n",
              " 94,\n",
              " 81,\n",
              " 100,\n",
              " 92,\n",
              " 81,\n",
              " 95,\n",
              " 83,\n",
              " 84,\n",
              " 81,\n",
              " 85,\n",
              " 100,\n",
              " 83,\n",
              " 89,\n",
              " 91,\n",
              " 85,\n",
              " 81,\n",
              " 84,\n",
              " 97,\n",
              " 80,\n",
              " 93,\n",
              " 95,\n",
              " 100,\n",
              " 83,\n",
              " 95,\n",
              " 91,\n",
              " 84,\n",
              " 98,\n",
              " 91,\n",
              " 87,\n",
              " 82,\n",
              " 80,\n",
              " 93,\n",
              " 88,\n",
              " 99,\n",
              " 87,\n",
              " 81,\n",
              " 96,\n",
              " 90,\n",
              " 87,\n",
              " 82,\n",
              " 93,\n",
              " 91,\n",
              " 87,\n",
              " 81,\n",
              " 87,\n",
              " 99,\n",
              " 87,\n",
              " 91,\n",
              " 83,\n",
              " 99,\n",
              " 90,\n",
              " 89,\n",
              " 99,\n",
              " 84,\n",
              " 93,\n",
              " 86,\n",
              " 100,\n",
              " 85,\n",
              " 83,\n",
              " 92,\n",
              " 88,\n",
              " 86,\n",
              " 88,\n",
              " 99,\n",
              " 92,\n",
              " 95,\n",
              " 90,\n",
              " 98,\n",
              " 96,\n",
              " 97,\n",
              " 92,\n",
              " 86,\n",
              " 100,\n",
              " 81,\n",
              " 98,\n",
              " 90,\n",
              " 96,\n",
              " 99,\n",
              " 100,\n",
              " 87,\n",
              " 94,\n",
              " 81,\n",
              " 99,\n",
              " 100,\n",
              " 86,\n",
              " 80,\n",
              " 100,\n",
              " 80,\n",
              " 96,\n",
              " 87,\n",
              " 83,\n",
              " 85,\n",
              " 95,\n",
              " 86,\n",
              " 89,\n",
              " 94,\n",
              " 92,\n",
              " 91,\n",
              " 96,\n",
              " 93,\n",
              " 91,\n",
              " 86,\n",
              " 85,\n",
              " 100,\n",
              " 98,\n",
              " 89,\n",
              " 86,\n",
              " 87,\n",
              " 95,\n",
              " 81,\n",
              " 96,\n",
              " 88,\n",
              " 80,\n",
              " 85,\n",
              " 91,\n",
              " 95,\n",
              " 86,\n",
              " 96,\n",
              " 80,\n",
              " 100,\n",
              " 80,\n",
              " 88,\n",
              " 97,\n",
              " 90,\n",
              " 90,\n",
              " 97,\n",
              " 90,\n",
              " 80,\n",
              " 85,\n",
              " 89,\n",
              " 92,\n",
              " 96,\n",
              " 88,\n",
              " 86,\n",
              " 81,\n",
              " 95,\n",
              " 82,\n",
              " 91,\n",
              " 81,\n",
              " 97,\n",
              " 90,\n",
              " 88,\n",
              " 100,\n",
              " 85,\n",
              " 97,\n",
              " 86,\n",
              " 81,\n",
              " 95,\n",
              " 86,\n",
              " 81,\n",
              " 89,\n",
              " 80,\n",
              " 80,\n",
              " 94,\n",
              " 90,\n",
              " 87,\n",
              " 84,\n",
              " 85,\n",
              " 97,\n",
              " 88,\n",
              " 94,\n",
              " 93,\n",
              " 92,\n",
              " 86,\n",
              " 86,\n",
              " 98,\n",
              " 95,\n",
              " 95,\n",
              " 81,\n",
              " 86,\n",
              " 83,\n",
              " 94,\n",
              " 92,\n",
              " 81,\n",
              " 92,\n",
              " 81,\n",
              " 94,\n",
              " 83,\n",
              " 88,\n",
              " 98,\n",
              " 89,\n",
              " 81,\n",
              " 98,\n",
              " 95,\n",
              " 92,\n",
              " 94,\n",
              " 93,\n",
              " 82,\n",
              " 86,\n",
              " 80,\n",
              " 91,\n",
              " 85,\n",
              " 87,\n",
              " 80,\n",
              " 93,\n",
              " 93,\n",
              " 88,\n",
              " 89,\n",
              " 82,\n",
              " 81,\n",
              " 80,\n",
              " 90,\n",
              " 89,\n",
              " 98,\n",
              " 84,\n",
              " 84,\n",
              " 83,\n",
              " 84,\n",
              " 89,\n",
              " 98,\n",
              " 81,\n",
              " 81,\n",
              " 89,\n",
              " 85,\n",
              " 81,\n",
              " 83,\n",
              " 89,\n",
              " 97,\n",
              " 85,\n",
              " 94,\n",
              " 93,\n",
              " 89,\n",
              " 84,\n",
              " 91,\n",
              " 99,\n",
              " 82,\n",
              " 88,\n",
              " 81,\n",
              " 98,\n",
              " 96,\n",
              " 92,\n",
              " 94,\n",
              " 93,\n",
              " 89,\n",
              " 99,\n",
              " 96,\n",
              " 85,\n",
              " 82,\n",
              " 88,\n",
              " 82,\n",
              " 99,\n",
              " 99,\n",
              " 99,\n",
              " 88,\n",
              " 97,\n",
              " 85,\n",
              " 89,\n",
              " 85,\n",
              " 81,\n",
              " 84,\n",
              " 100,\n",
              " 80,\n",
              " 90,\n",
              " 96,\n",
              " 92,\n",
              " 89,\n",
              " 84,\n",
              " 90,\n",
              " 89,\n",
              " 89,\n",
              " 92,\n",
              " 92,\n",
              " 84,\n",
              " 89,\n",
              " 85,\n",
              " 91,\n",
              " 91,\n",
              " 90,\n",
              " 95,\n",
              " 88,\n",
              " 95,\n",
              " 95,\n",
              " 100,\n",
              " 92,\n",
              " 91,\n",
              " 83,\n",
              " 93,\n",
              " 91,\n",
              " 95,\n",
              " 92,\n",
              " 82,\n",
              " 94,\n",
              " 93,\n",
              " 96,\n",
              " 91,\n",
              " 92,\n",
              " 89,\n",
              " 92,\n",
              " 81,\n",
              " 91,\n",
              " 84,\n",
              " 96,\n",
              " 93,\n",
              " 93,\n",
              " 97,\n",
              " 95,\n",
              " 81,\n",
              " 89,\n",
              " 91,\n",
              " 95,\n",
              " 84,\n",
              " 88,\n",
              " 86,\n",
              " 86,\n",
              " 96,\n",
              " 86,\n",
              " 97,\n",
              " 88,\n",
              " 88,\n",
              " 95,\n",
              " 90,\n",
              " 91,\n",
              " 87,\n",
              " 88,\n",
              " 95,\n",
              " 95,\n",
              " 86,\n",
              " 100,\n",
              " 95,\n",
              " 90,\n",
              " 81,\n",
              " 88,\n",
              " 85,\n",
              " 94,\n",
              " 84,\n",
              " 100,\n",
              " 100,\n",
              " 95,\n",
              " 91,\n",
              " 90,\n",
              " 85,\n",
              " 88,\n",
              " 97,\n",
              " 83,\n",
              " 95,\n",
              " 84,\n",
              " 97,\n",
              " 88,\n",
              " 83,\n",
              " 95,\n",
              " 96,\n",
              " 93,\n",
              " 91,\n",
              " 97,\n",
              " 88,\n",
              " 94,\n",
              " 93,\n",
              " 89,\n",
              " 92,\n",
              " 83,\n",
              " 90,\n",
              " 97,\n",
              " 87,\n",
              " 87,\n",
              " 93,\n",
              " 100,\n",
              " 98,\n",
              " 98,\n",
              " 90,\n",
              " 86,\n",
              " 86,\n",
              " 99,\n",
              " 82,\n",
              " 99,\n",
              " 96,\n",
              " 96,\n",
              " 99,\n",
              " 84,\n",
              " 84,\n",
              " 99,\n",
              " 95,\n",
              " 86,\n",
              " 84,\n",
              " 83,\n",
              " 81,\n",
              " 87,\n",
              " 100,\n",
              " 88,\n",
              " 88,\n",
              " 91,\n",
              " 85,\n",
              " 83,\n",
              " 86,\n",
              " 86,\n",
              " 81,\n",
              " 99,\n",
              " 99,\n",
              " 95,\n",
              " 91,\n",
              " 82,\n",
              " 83,\n",
              " 86,\n",
              " 83,\n",
              " 98,\n",
              " 96,\n",
              " 87,\n",
              " 89,\n",
              " 92,\n",
              " 90,\n",
              " 100,\n",
              " 85,\n",
              " 87,\n",
              " 99,\n",
              " 95,\n",
              " 82,\n",
              " 85,\n",
              " 91,\n",
              " 80,\n",
              " 82,\n",
              " 96,\n",
              " 100,\n",
              " 100,\n",
              " 85,\n",
              " 87,\n",
              " 89,\n",
              " 98,\n",
              " 83,\n",
              " 84,\n",
              " 80,\n",
              " 92,\n",
              " 97,\n",
              " 88,\n",
              " 84,\n",
              " 84,\n",
              " 82,\n",
              " 97,\n",
              " 97,\n",
              " 83,\n",
              " 84,\n",
              " 84,\n",
              " 81,\n",
              " 86,\n",
              " 94,\n",
              " 88,\n",
              " 99,\n",
              " 82,\n",
              " 94,\n",
              " 89,\n",
              " 96,\n",
              " 98,\n",
              " 100,\n",
              " 90,\n",
              " 86,\n",
              " 85,\n",
              " 97,\n",
              " 83,\n",
              " 100,\n",
              " 81,\n",
              " 88,\n",
              " 84,\n",
              " 93,\n",
              " 80,\n",
              " 92,\n",
              " 97,\n",
              " 91,\n",
              " 88,\n",
              " 95,\n",
              " 99,\n",
              " 90,\n",
              " 84,\n",
              " 86,\n",
              " 91,\n",
              " 91,\n",
              " 99,\n",
              " 84,\n",
              " 94,\n",
              " 83,\n",
              " 85,\n",
              " 91,\n",
              " 81,\n",
              " 95,\n",
              " 90,\n",
              " 83,\n",
              " 82,\n",
              " 83,\n",
              " 85,\n",
              " 81,\n",
              " 89,\n",
              " 81,\n",
              " 87,\n",
              " 94,\n",
              " 86,\n",
              " 100,\n",
              " 94,\n",
              " 85,\n",
              " 93,\n",
              " 100,\n",
              " 87,\n",
              " 81,\n",
              " 95,\n",
              " 94,\n",
              " 84,\n",
              " 92,\n",
              " 100,\n",
              " 100,\n",
              " 84,\n",
              " 97,\n",
              " 84,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1t3bDkEAQQd"
      },
      "source": [
        "del essays\n",
        "del topics"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k43UVRcYmj49"
      },
      "source": [
        "**Batch data iterator**\n",
        "\n",
        "We want to iter through training data in batches and feed them into the network, and this is how we prepare for it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvC2OCOLmm39"
      },
      "source": [
        "def data_iterator(corpus_indice, topics_indice, batch_size, num_steps):\n",
        "    epoch_size = len(corpus_indice) // batch_size\n",
        "    for i in range(epoch_size):\n",
        "        raw_data = corpus_indice[i*batch_size: (i+1)*batch_size]\n",
        "        key_words = topics_indice[i*batch_size: (i+1)*batch_size]\n",
        "        data = np.zeros((len(raw_data), num_steps+1), dtype=np.int64)\n",
        "        # data = np.zeros((len(raw_data), num_steps+1), dtype=np.int32)\n",
        "        for i in range(batch_size):\n",
        "            doc = raw_data[i]\n",
        "            tmp = [1]\n",
        "            tmp.extend(doc)\n",
        "            tmp.extend([2])\n",
        "            tmp = np.array(tmp, dtype=np.int64)\n",
        "            # tmp = np.array(tmp, dtype=np.int32)\n",
        "            _size = tmp.shape[0]\n",
        "            data[i][:_size] = tmp\n",
        "        key_words = np.array(key_words, dtype=np.int64)\n",
        "        # key_words = np.array(key_words, dtype=np.int32)\n",
        "        x = data[:, 0:num_steps]\n",
        "        y = data[:, 1:]\n",
        "        mask = np.float32(x != 0)\n",
        "        x = torch.tensor(x)\n",
        "        y = torch.tensor(y)\n",
        "        mask = torch.tensor(mask)\n",
        "        key_words = torch.tensor(key_words)\n",
        "        yield(x, y, mask, key_words)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JCT2YWwpu3b"
      },
      "source": [
        "**Build model: MTA-LSTM**\n",
        "This is the most important part in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByYAvJYNp0l_"
      },
      "source": [
        "**Bahdanau Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHITw0npAQTY"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, embed_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.Ua = nn.Linear(embed_size, hidden_size, bias=False)\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.va = nn.Linear(hidden_size, 1, bias=True)\n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query, topics, coverage_vector):\n",
        "        scores = []\n",
        "        C_t = coverage_vector.clone()\n",
        "        for i in range(topics.shape[1]):\n",
        "            proj_key = self.Ua(topics[:, i, :])\n",
        "            query = self.Wa(query)\n",
        "            scores += [self.va(torch.tanh(query + proj_key)) * C_t[:, i:i+1]]\n",
        "            \n",
        "        # stack scores\n",
        "        scores = torch.stack(scores, dim=1)\n",
        "        scores = scores.squeeze(2)\n",
        "#         print(scores.shape)\n",
        "        # turn scores to probabilities\n",
        "        alphas = F.softmax(scores, dim=1)\n",
        "        self.alphas = alphas\n",
        "        \n",
        "        # mt vector is the weighted sum of the topics\n",
        "        mt = torch.bmm(alphas.unsqueeze(1), topics)\n",
        "        mt = mt.squeeze(1)\n",
        "        \n",
        "        # mt shape: [batch x embed], alphas shape: [batch x num_keywords]\n",
        "        return mt, alphas"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GHF_Hrgp6DC"
      },
      "source": [
        "**Attention Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLLC1nACp4GL"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embed_size, num_layers, dropout=0.5):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # topic attention\n",
        "        self.attention = Attention(hidden_size, embed_size)\n",
        "        \n",
        "        # lstm\n",
        "        self.rnn = nn.LSTM(input_size=embed_size * 2, \n",
        "                           hidden_size=hidden_size, \n",
        "                           num_layers=num_layers, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "    def forward(self, input, output, hidden, phi, topics, coverage_vector):\n",
        "        # 1. calculate attention weight and mt\n",
        "        mt, score = self.attention(output.squeeze(0), topics, coverage_vector)\n",
        "        mt = mt.unsqueeze(1).permute(1, 0, 2)\n",
        "        \n",
        "        # 2. update coverge vector [batch x num_keywords]\n",
        "        coverage_vector = coverage_vector - score / phi\n",
        "        \n",
        "        # 3. concat input and Tt, and feed into rnn \n",
        "        output, hidden = self.rnn(torch.cat([input, mt], dim=2), hidden)\n",
        "        \n",
        "        return output, hidden, score, coverage_vector"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHGpiGw1p_X6"
      },
      "source": [
        "**MTA-LSTM model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_QnRvSDp9Rx"
      },
      "source": [
        "LSTMState = namedtuple('LSTMState', ['hx', 'cx'])\n",
        "\n",
        "class MTALSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim, embed_dim, num_keywords, num_layers, weight,\n",
        "                 num_labels, bidirectional, dropout=0.5, **kwargs):\n",
        "        super(MTALSTM, self).__init__(**kwargs)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_labels = num_labels\n",
        "        self.bidirectional = bidirectional\n",
        "        if num_layers <= 1:\n",
        "            self.dropout = 0\n",
        "        else:\n",
        "            self.dropout = dropout\n",
        "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.Uf = nn.Linear(embed_dim * num_keywords, num_keywords, bias=False)\n",
        "        \n",
        "        # attention decoder\n",
        "        self.decoder = AttentionDecoder(hidden_size=hidden_dim, \n",
        "                                        embed_size=embed_dim, \n",
        "                                        num_layers=num_layers, \n",
        "                                        dropout=dropout)\n",
        "        \n",
        "        # adaptive softmax\n",
        "        self.adaptiveSoftmax = nn.AdaptiveLogSoftmaxWithLoss(hidden_dim, \n",
        "                                                             num_labels, \n",
        "                                                             cutoffs=[round(num_labels / 20), 4*round(num_labels / 20)])\n",
        "    \n",
        "    def forward(self, inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        topics_embed = self.embedding(topics)\n",
        "        ''' calculate phi [batch x num_keywords] '''\n",
        "        phi = None\n",
        "        phi = torch.sum(mask, dim=1, keepdim=True) * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
        "        \n",
        "        # loop through sequence\n",
        "        inputs = embeddings.permute([1, 0, 2]).unbind(0)\n",
        "        output_states = []\n",
        "        attn_weight = []\n",
        "        for i in range(len(inputs)):\n",
        "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
        "                                                                        output=output, \n",
        "                                                                        hidden=hidden, \n",
        "                                                                        phi=phi, \n",
        "                                                                        topics=topics_embed, \n",
        "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
        "            output_states += [output]\n",
        "            attn_weight += [score]\n",
        "            \n",
        "        output_states = torch.stack(output_states)\n",
        "        attn_weight = torch.stack(attn_weight)\n",
        "        \n",
        "        # calculate loss py adaptiveSoftmax\n",
        "        outputs = self.adaptiveSoftmax(output_states.reshape(-1, output_states.shape[-1]), target.t().reshape((-1,)))\n",
        "        \n",
        "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
        "    \n",
        "    def inference(self, inputs, topics, output, hidden=None, mask=None, coverage_vector=None, seq_length=None):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        topics_embed = self.embedding(topics)\n",
        "       \n",
        "        phi = None\n",
        "        phi = seq_length.float() * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
        "        \n",
        "        queries = embeddings.permute([1, 0, 2])[-1].unsqueeze(0)\n",
        "        \n",
        "        inputs = queries.permute([1, 0, 2]).unbind(0)\n",
        "        output_states = []\n",
        "        attn_weight = []\n",
        "        for i in range(len(inputs)):\n",
        "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
        "                                                                        output=output, \n",
        "                                                                        hidden=hidden, \n",
        "                                                                        phi=phi, \n",
        "                                                                        topics=topics_embed, \n",
        "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
        "            output_states += [output]\n",
        "            attn_weight += [score]\n",
        "            \n",
        "        output_states = torch.stack(output_states)\n",
        "        attn_weight = torch.stack(attn_weight)\n",
        "        \n",
        "        outputs = self.adaptiveSoftmax.log_prob(output_states.reshape(-1, output_states.shape[-1]))\n",
        "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device), \n",
        "                  torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "        return hidden\n",
        "    \n",
        "    def init_coverage_vector(self, batch_size, num_keywords):\n",
        "        return torch.ones([batch_size, num_keywords]).to(device)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1zdczCqFmo"
      },
      "source": [
        "**Greedy decode strategy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtA8t2qmqEIj"
      },
      "source": [
        "\n",
        "def pad_topic(topics):\n",
        "    topics = [word_to_idx[x] for x in topics]\n",
        "    ###.to(device) added in below line by me\n",
        "    topics = torch.tensor(topics).to(device)\n",
        "    print(topics)\n",
        "    max_num = 5     # no. of topics you are using, change accordingly\n",
        "    size = 1\n",
        "    ans = np.zeros((size, max_num), dtype=int)\n",
        "    for i in range(size):\n",
        "        true_len = min(len(topics), max_num)\n",
        "        for j in range(true_len):\n",
        "            print(topics[i])\n",
        "            ans[i][j] = topics[i][j]\n",
        "    return ans"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBrJ5PpX9Rbu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJfkrHjRqJaH"
      },
      "source": [
        "\n",
        "def predict_rnn(topics, num_chars, model, idx_to_word, word_to_idx):\n",
        "    output_idx = [1]\n",
        "    topics = [word_to_idx[x] for x in topics]\n",
        "    topics = torch.tensor(topics)\n",
        "    topics = topics.reshape((1, topics.shape[0]))\n",
        "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
        "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    if use_gpu:\n",
        "#         hidden = hidden.cuda()\n",
        "        adaptive_softmax.to(device)\n",
        "        topics = topics.to(device)\n",
        "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
        "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
        "    for t in range(num_chars):\n",
        "        X = torch.tensor(output_idx[-1]).reshape((1, 1))\n",
        "#         X = torch.tensor(output).reshape((1, len(output)))\n",
        "        if use_gpu:\n",
        "            X = X.to(device)\n",
        "        if t == 0:\n",
        "            output = torch.zeros(1, hidden_dim).to(device)\n",
        "        else:\n",
        "            output = output.squeeze(0)\n",
        "        # May be need to change seq length in line below\n",
        "        pred, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, topics=topics, output=output, hidden=hidden, coverage_vector=coverage_vector, seq_length=torch.tensor(50).reshape(1, 1).to(device))\n",
        "#         print(coverage_vector)\n",
        "        pred = pred.argmax(dim=1) # greedy strategy\n",
        "        attentions[t] = attn_weight[0].data\n",
        "#         pred = adaptive_softmax.predict(pred)\n",
        "        if pred[-1] == 2:\n",
        "#         if pred.argmax(dim=1)[-1] == 2:\n",
        "            break\n",
        "        else:\n",
        "            output_idx.append(int(pred[-1]))\n",
        "#             output.append(int(pred.argmax(dim=1)[-1]))\n",
        "    return(''.join([idx_to_word[i] for i in output_idx[1:]]), [idx_to_word[i] for i in output_idx[1:]], attentions[:t+1].t(), output_idx[1:])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq6TJGeYqLfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30eeab8d-54e4-405b-c847-02b1ea79d17b"
      },
      "source": [
        "test = [1, 15, 23]\n",
        "test = np.array(test, dtype=np.int64)\n",
        "mm = np.float32(test != 0)\n",
        "mm"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCVEKsEOqQXP"
      },
      "source": [
        "**Beam search strategy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvRA7_YQqN8O"
      },
      "source": [
        "def beam_search(topics, num_chars, model, idx_to_word, word_to_idx, is_sample=False):\n",
        "    output_idx = [1]\n",
        "    topics = [word_to_idx[x] for x in topics]\n",
        "    topics = torch.tensor(topics)\n",
        "    topics = topics.reshape((1, topics.shape[0]))\n",
        "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
        "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    if use_gpu:\n",
        "#         hidden = hidden.cuda()\n",
        "        adaptive_softmax.to(device)\n",
        "        topics = topics.to(device)\n",
        "        # Also change seq_length here\n",
        "        seq_length = torch.tensor(50).reshape(1, 1).to(device)\n",
        "    \"\"\"1\"\"\"    \n",
        "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
        "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
        "    X = torch.tensor(output_idx[-1]).reshape((1, 1)).to(device)\n",
        "    output = torch.zeros(1, hidden_dim).to(device)\n",
        "    log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
        "                                                                   topics=topics, \n",
        "                                                                   output=output, \n",
        "                                                                   hidden=hidden, \n",
        "                                                                   coverage_vector=coverage_vector, \n",
        "                                                                   seq_length=seq_length)\n",
        "    log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
        "#     print(log_prob[10])\n",
        "    \"\"\"2\"\"\"\n",
        "    if is_sample:\n",
        "        top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
        "    else:\n",
        "        top_indices = np.argsort(-log_prob)\n",
        "    \"\"\"3\"\"\"\n",
        "    beams = [(0.0, [idx_to_word[1]], idx_to_word[1], torch.zeros(1, topics.shape[1]), torch.ones(1, topics.shape[1]))]\n",
        "    b = beams[0]\n",
        "    beam_candidates = []\n",
        "#     print(attn_weight[0].cpu().data, coverage_vector)\n",
        "#     assert False\n",
        "    for i in range(beam_size):\n",
        "        word_idx = top_indices[i]\n",
        "        beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
        "    \"\"\"4\"\"\"\n",
        "    beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
        "    beams = beam_candidates[:beam_size] # truncate to get new beams\n",
        "    \n",
        "    for xy in range(num_chars-1):\n",
        "        beam_candidates = []\n",
        "        for b in beams:\n",
        "            \"\"\"5\"\"\"\n",
        "            X = torch.tensor(b[2]).reshape((1, 1)).to(device)\n",
        "            \"\"\"6\"\"\"\n",
        "            log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
        "                                                                           topics=topics, \n",
        "                                                                           output=b[6], \n",
        "                                                                           hidden=b[5], \n",
        "                                                                           coverage_vector=b[7], \n",
        "                                                                           seq_length=seq_length)\n",
        "            log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
        "            \"\"\"8\"\"\"\n",
        "            if is_sample:\n",
        "                top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
        "            else:\n",
        "                top_indices = np.argsort(-log_prob)\n",
        "            \"\"\"9\"\"\"\n",
        "            for i in range(beam_size):\n",
        "                word_idx = top_indices[i]\n",
        "                beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
        "        \"\"\"10\"\"\"\n",
        "        beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
        "        beams = beam_candidates[:beam_size] # truncate to get new beams\n",
        "    \n",
        "    \"\"\"11\"\"\"\n",
        "    if '<EOS>' in beams[0][1]:\n",
        "        first_eos = beams[0][1].index('<EOS>')\n",
        "    else:\n",
        "        first_eos = num_chars-1\n",
        "    return(''.join(beams[0][1][:first_eos]), beams[0][1][:first_eos], beams[0][3][:first_eos].t(), beams[0][4][:first_eos])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2pSvAXWqXNU"
      },
      "source": [
        "**Attention visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCmIFA3wqUZx"
      },
      "source": [
        "# plt.switch_backend('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "    \n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.subplots(1)\n",
        "#     cmap = 'bone'\n",
        "    cmap = 'viridis'\n",
        "    cax = ax.matshow(attentions.numpy(), cmap=cmap)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    # ax.set_yticklabels([''] + input_sentence.split(' '), fontproperties=fontprop, fontsize=10)\n",
        "    # ax.set_xticklabels([''] + output_words, fontproperties=fontprop, fontsize=10, rotation=45)\n",
        "\n",
        "    ax.set_yticklabels([''] + input_sentence.split(' '))\n",
        "    ax.set_xticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    \n",
        "    word_size = 0.5\n",
        "    fig.set_figheight(word_size * len(input_sentence.split(' ')))\n",
        "    fig.set_figwidth(word_size * len(output_words))\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence, method='beam_search', is_sample=False):\n",
        "    num_chars = 100     # change this to set size of output\n",
        "    if method == 'beam_search':\n",
        "        _, output_words, attentions, coverage_vector = beam_search(input_sentence, num_chars, model, idx_to_word, word_to_idx, is_sample=is_sample)\n",
        "    else:\n",
        "        _, output_words, attentions, _ = predict_rnn(input_sentence, num_chars, model, idx_to_word, word_to_idx)\n",
        "    #print('input =', ' '.join(input_sentence))\n",
        "    #print('output =', ' '.join(output_words))\n",
        "#     n_digits = 3\n",
        "#     coverage_vector = torch.round(coverage_vector * 10**n_digits) / (10**n_digits)\n",
        "#     coverage_vector=np.round(coverage_vector, n_digits)\n",
        "#     print(coverage_vector.numpy())\n",
        "\n",
        "    showAttention(' '.join(input_sentence), output_words, attentions)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-b-cI4qbI3"
      },
      "source": [
        "**Bleu score calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4AWd3wpqajE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9246c8d4-e0d5-4588-9c15-c09d16072759"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(model, topics_test, corpus_test, num_test, method='beam_search', is_sample=False):\n",
        "    num_chars = 100     # change this to set size of output\n",
        "    bleu_2_score = 0\n",
        "    for i in tqdm(range(len(corpus_test[:num_test]))):\n",
        "        if method == 'beam_search':\n",
        "            _, output_words, _, _ = beam_search([idx_to_word[x] for x in topics_test[i]], num_chars, model, idx_to_word, word_to_idx, False)\n",
        "        else:\n",
        "            _, output_words, _, _ = predict_rnn([idx_to_word[x] for x in topics_test[i]], num_chars, model, idx_to_word, word_to_idx)\n",
        "        bleu_2_score += sentence_bleu([[idx_to_word[x] for x in corpus_test[i] if x not in [0, 2]]], output_words, weights=(0, 1, 0, 0))\n",
        "        \n",
        "    bleu_2_score = bleu_2_score / num_test\n",
        "    return bleu_2_score\n",
        "\n",
        "print([[idx_to_word[x] for x in corpus_test[10] if x not in [0, 2]]])\n",
        "print(len(corpus_test[10]))\n",
        "print(len([ idx_to_word[x] for x in corpus_test[10] ]))\n",
        "print(len(corpus_test[:100]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['تھا', 'چھوٹے', 'ماموں', 'نے', 'ایک', 'دم', 'اچھل', 'کر', 'کہا', '۔', 'آہاہا', '،', 'ہاں', '،', '!', 'ہم', 'سب', 'خوشی', 'کے', 'مارے', 'آنکھیں', 'پھاڑ', 'پھاڑ', 'کر', 'ایک', 'دوسرے', 'کی', 'طرف', 'دیکھنے', 'لگے', '۔', 'مگر', 'اسٹور', 'تک', 'پہنچنے', 'کے', 'لیے', 'صرف', 'ابا', 'کا', 'کمرہ', 'ہی', 'راستہ', 'تھا', 'ماموں', 'کٹورا', 'لے', 'کر', 'ننگے', 'پاو\\x04¿ں', 'بڑھے', 'اچانک', 'دیدی', 'بولی', ':', 'سب', 'کتابوں', 'میں', 'لکھا', 'ہے', 'جوری', 'بری', 'بات', 'ہے', '۔', 'ہٹ', '!', 'غریب', 'کے', 'لیے', 'چوری', 'کرنے', 'میں', 'گناہ', 'نہیں', 'ہوتا', '،', 'وہ', 'جو', 'سلطانہ', 'ڈاکو', 'تھا', 'وہ', 'سب', 'لوٹ', 'کر', 'غریبوں', 'میں', 'بانٹ', 'دیا', 'کرتا', 'تھا', 'میں']]\n",
            "93\n",
            "93\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb3lF9DSqiPQ"
      },
      "source": [
        "**Some configurations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHw3Vb4mqfax"
      },
      "source": [
        "embedding_dim = 100     # depends on your Word2Vec model embedding size\n",
        "# hidden_dim = 512\n",
        "hidden_dim = 128\n",
        "lr = 1e-3 * 0.5\n",
        "momentum = 0.01\n",
        "\n",
        "\n",
        "num_epoch = 80\n",
        "\n",
        "clip_value = 0.1\n",
        "use_gpu = True\n",
        "# use_gpu = False\n",
        "num_layers = 2\n",
        "# num_layers = 1\n",
        "bidirectional = False\n",
        "# batch_size = 32\n",
        "batch_size = 4\n",
        "num_keywords = 5            # change this if more keywords\n",
        "verbose = 1\n",
        "check_point = 5\n",
        "beam_size = 2\n",
        "is_sample = True\n",
        "vocab_size = len(vocab)\n",
        "# device = torch.device(deviceName)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
        "    hidden_dim, len(vocab), cutoffs=[round(vocab_size / 20), 4*round(vocab_size / 20)])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvRqnQYarRLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b30959-3c29-46b8-9470-65b61d61a263"
      },
      "source": [
        "!pip install keras-rectified-adam"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rectified-adam\n",
            "  Downloading https://files.pythonhosted.org/packages/35/6f/e91e36f09178df814ccfc4f6fc7239175cb6b67ce5fe9fbb579152aa326a/keras-rectified-adam-0.18.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-rectified-adam) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-rectified-adam) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-rectified-adam) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-rectified-adam) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-rectified-adam) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-rectified-adam) (1.15.0)\n",
            "Building wheels for collected packages: keras-rectified-adam\n",
            "  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rectified-adam: filename=keras_rectified_adam-0.18.0-cp37-none-any.whl size=8950 sha256=e8e2fde2437bb8d33d2c37f485148b73cc9899a175573cda773bc0a03538786a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/7f/90/bdcf51356f0efcc737e0fb1d5ea310418ae4c78b4614fbc67e\n",
            "Successfully built keras-rectified-adam\n",
            "Installing collected packages: keras-rectified-adam\n",
            "Successfully installed keras-rectified-adam-0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McAsOnSrRGFL"
      },
      "source": [
        "model = MTALSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_keywords=num_keywords, \n",
        "                num_layers=num_layers, num_labels=len(vocab), weight=word_vec, bidirectional=bidirectional)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxXCZc2yqlwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5316a05-c84e-43dc-d49e-3678144b1a76"
      },
      "source": [
        "model = MTALSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_keywords=num_keywords, \n",
        "                num_layers=num_layers, num_labels=len(vocab), weight=word_vec, bidirectional=bidirectional)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "# uncommented by me \n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#from keras_radam import RAdam\n",
        "#optimizer = RAdam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
        "# optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "if use_gpu:\n",
        "#     model = nn.DataParallel(model)\n",
        "#     model = model.to(device)\n",
        "  model = model.to('cuda:0')\n",
        "  print(\"Dump to cuda\")\n",
        "else:\n",
        "  model = model.to(device)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dump to cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQFrwUTnqo2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b70aceb-7720-44d6-99b0-49b2ea847f56"
      },
      "source": [
        "\n",
        "def params_init_uniform(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        y = 0.04\n",
        "        nn.init.uniform_(m.weight, -y, y)\n",
        "        \n",
        "model.apply(params_init_uniform)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTALSTM(\n",
              "  (embedding): Embedding(146817, 100)\n",
              "  (Uf): Linear(in_features=500, out_features=5, bias=False)\n",
              "  (decoder): AttentionDecoder(\n",
              "    (attention): Attention(\n",
              "      (Ua): Linear(in_features=100, out_features=128, bias=False)\n",
              "      (Wa): Linear(in_features=128, out_features=128, bias=False)\n",
              "      (va): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (rnn): LSTM(200, 128, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (adaptiveSoftmax): AdaptiveLogSoftmaxWithLoss(\n",
              "    (head): Linear(in_features=128, out_features=7343, bias=False)\n",
              "    (tail): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=32, bias=False)\n",
              "        (1): Linear(in_features=32, out_features=22023, bias=False)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=8, bias=False)\n",
              "        (1): Linear(in_features=8, out_features=117453, bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYYb-qSlzmre"
      },
      "source": [
        "# Load previous checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1skxKSw5ugIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b650f68b-d0a7-4cb3-ecea-67c51b87bdeb"
      },
      "source": [
        "version_num = 1\n",
        "# Type = 'best'\n",
        "Type = 'trainable'\n",
        "model_check_point = '%s/model_%s_%d.pk' % (save_folder, Type, version_num)\n",
        "optim_check_point = '%s/optim_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "loss_check_point = '%s/loss_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "epoch_check_point = '%s/epoch_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "bleu_check_point = '%s/bleu_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "loss_values = []\n",
        "epoch_values = []\n",
        "bleu_values = []\n",
        "if os.path.isfile(model_check_point):\n",
        "    print('Loading previous status (ver.%d)...' % version_num)\n",
        "    model.load_state_dict(torch.load(model_check_point, map_location='cpu'))\n",
        "    model = model.to(device)\n",
        "    optimizer.load_state_dict(torch.load(optim_check_point))\n",
        "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
        "    loss_values = torch.load(loss_check_point)\n",
        "    epoch_values = torch.load(epoch_check_point)\n",
        "    bleu_values = torch.load(bleu_check_point)\n",
        "    print('Load successfully')\n",
        "else:\n",
        "    print(\"ver.%d doesn't exist\" % version_num)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading previous status (ver.1)...\n",
            "Load successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-I85WxrvELj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a020045-f806-49ea-be7b-35e69d2a72cd"
      },
      "source": [
        "def isnan(x):\n",
        "    return x != x\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "#     if p.grad is None:\n",
        "#         continue\n",
        "    if p.requires_grad:\n",
        "        print(name, p)\n",
        "#         p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uf.weight Parameter containing:\n",
            "tensor([[-0.0443, -0.1080,  0.0762,  ...,  0.1006,  0.0372, -0.0090],\n",
            "        [ 0.0621, -0.0320, -0.0580,  ...,  0.0659, -0.0534, -0.0218],\n",
            "        [ 0.0103,  0.0520, -0.0090,  ...,  0.0262,  0.0397, -0.0869],\n",
            "        [-0.0251,  0.1063, -0.0021,  ...,  0.0074, -0.1113, -0.0216],\n",
            "        [-0.0430, -0.1499, -0.0318,  ...,  0.0165, -0.0107, -0.0323]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.attention.Ua.weight Parameter containing:\n",
            "tensor([[ 0.0875,  0.1000,  0.2211,  ..., -0.1830, -0.1067, -0.0955],\n",
            "        [ 0.0010,  0.2574, -0.2827,  ...,  0.0358, -0.0143,  0.1897],\n",
            "        [ 0.0322, -0.2989,  0.2179,  ..., -0.0422,  0.0434, -0.0881],\n",
            "        ...,\n",
            "        [ 0.0051,  0.2908, -0.2869,  ...,  0.0465, -0.0956,  0.1361],\n",
            "        [-0.1012,  0.0736, -0.2038,  ...,  0.1308,  0.0572,  0.1367],\n",
            "        [-0.0212,  0.3074, -0.2535,  ...,  0.0589, -0.0762,  0.1670]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.attention.Wa.weight Parameter containing:\n",
            "tensor([[-0.1146,  0.0999, -0.0666,  ...,  0.0425,  0.0799, -0.0576],\n",
            "        [ 0.0066,  0.1345, -0.0279,  ...,  0.0215, -0.0699,  0.1411],\n",
            "        [-0.1160,  0.1267, -0.1270,  ...,  0.1014, -0.0589,  0.0671],\n",
            "        ...,\n",
            "        [ 0.1884, -0.1456,  0.1240,  ..., -0.1726,  0.0227, -0.0259],\n",
            "        [ 0.0875, -0.0523,  0.0741,  ..., -0.1061, -0.0953,  0.0211],\n",
            "        [ 0.0261,  0.0890, -0.0420,  ...,  0.0047, -0.1040,  0.1747]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.attention.va.weight Parameter containing:\n",
            "tensor([[ 0.0508, -0.1045,  0.0483,  0.0121,  0.0592, -0.0560,  0.0129, -0.0395,\n",
            "          0.0720, -0.0272, -0.0660,  0.0542, -0.0563, -0.0575, -0.0320, -0.0807,\n",
            "         -0.0692,  0.0857, -0.0760,  0.0850,  0.0816, -0.0807, -0.0153, -0.1026,\n",
            "         -0.0627, -0.0694,  0.0973,  0.0533, -0.0853,  0.0914,  0.0405, -0.0738,\n",
            "          0.0162,  0.0929,  0.0565, -0.0844, -0.0727, -0.0480,  0.0762,  0.0503,\n",
            "         -0.0999, -0.0673,  0.0482,  0.0702,  0.0965,  0.0519,  0.0608, -0.0404,\n",
            "          0.0358,  0.0576,  0.0664, -0.1041, -0.0430,  0.0605, -0.0522,  0.0593,\n",
            "          0.0512, -0.0740,  0.0732,  0.0102,  0.0881,  0.0526,  0.0989, -0.0588,\n",
            "         -0.1092, -0.0376, -0.0830,  0.0797, -0.0537,  0.0383,  0.0456, -0.0755,\n",
            "         -0.0532,  0.0641,  0.0584, -0.0666, -0.0695, -0.0598,  0.1069, -0.0906,\n",
            "          0.0906,  0.0695,  0.0806,  0.0607,  0.0629, -0.0419, -0.0490, -0.0518,\n",
            "          0.0808, -0.0303, -0.0956,  0.0577,  0.0607, -0.0852, -0.0216,  0.1076,\n",
            "          0.0594,  0.0557,  0.0651, -0.0890, -0.0334,  0.0611,  0.1016, -0.0545,\n",
            "         -0.0567, -0.0600, -0.0928, -0.0729, -0.0729, -0.0503, -0.0836,  0.0745,\n",
            "          0.0846,  0.0459, -0.0400,  0.0175,  0.0366,  0.0638,  0.0495,  0.0031,\n",
            "         -0.0809,  0.0662,  0.0717,  0.0396,  0.0566, -0.0727, -0.0338, -0.0996]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.attention.va.bias Parameter containing:\n",
            "tensor([0.0257], device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.weight_ih_l0 Parameter containing:\n",
            "tensor([[-0.1568, -0.0885,  0.0338,  ...,  0.0163, -0.2101, -0.1729],\n",
            "        [ 0.1181, -0.1998, -0.0979,  ..., -0.0260, -0.1829, -0.1996],\n",
            "        [-0.0402, -0.3338, -0.0389,  ..., -0.1137, -0.0499, -0.0146],\n",
            "        ...,\n",
            "        [ 1.3298,  0.4042,  0.5954,  ...,  0.1433,  0.1682, -0.0408],\n",
            "        [ 0.8204,  0.3791,  0.4386,  ...,  0.0565,  0.2015,  0.0887],\n",
            "        [-0.3975, -0.3147, -0.5046,  ..., -0.0170, -0.0670,  0.1230]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.weight_hh_l0 Parameter containing:\n",
            "tensor([[-0.0370, -0.0795, -0.0977,  ..., -0.0430,  0.2187,  0.1758],\n",
            "        [-0.0811, -0.2776, -0.1303,  ...,  0.1429, -0.1509,  0.3012],\n",
            "        [-0.0916, -0.0325, -0.0450,  ..., -0.0803,  0.2053,  0.1142],\n",
            "        ...,\n",
            "        [ 0.1672, -0.0755,  0.0395,  ...,  0.2814, -0.0366,  0.0304],\n",
            "        [ 0.0549, -0.0059,  0.1931,  ...,  0.4564, -0.0763, -0.0676],\n",
            "        [-0.1532,  0.0261,  0.0229,  ..., -0.0849,  0.0620,  0.1639]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.bias_ih_l0 Parameter containing:\n",
            "tensor([ 9.8535e-02,  1.7389e-01,  9.5783e-02,  5.9125e-02,  1.2891e-01,\n",
            "        -1.6171e-02,  3.9647e-02,  9.4559e-02,  1.8495e-02,  9.4540e-02,\n",
            "         1.0116e-01,  1.0809e-01,  2.0305e-01, -3.2443e-02,  1.8228e-01,\n",
            "         2.4271e-01,  6.0518e-02,  1.0427e-01,  1.6607e-01,  2.3593e-01,\n",
            "         4.0744e-01,  2.2946e-01,  6.9335e-02,  7.8745e-02,  1.6026e-02,\n",
            "         1.3246e-01, -3.2431e-02,  1.8622e-01,  3.1458e-01,  1.6895e-01,\n",
            "         1.6366e-01,  1.9508e-01,  1.1703e-01,  5.4766e-02,  2.7173e-01,\n",
            "         3.7221e-02,  2.8978e-02,  1.8781e-01,  8.2320e-02, -8.1651e-02,\n",
            "         1.0539e-01,  2.2399e-01,  1.6627e-01,  1.0821e-01,  1.0799e-01,\n",
            "         2.0370e-01,  1.4231e-01,  9.3342e-02,  2.3606e-01,  1.5784e-01,\n",
            "         8.0885e-02,  3.0246e-02,  1.6628e-01,  2.7702e-01,  6.8631e-02,\n",
            "        -1.2579e-01,  1.9493e-01,  5.0236e-01,  1.8894e-03,  7.6501e-02,\n",
            "         7.5459e-02,  4.7778e-02,  1.1474e-01,  1.6127e-02, -4.0952e-02,\n",
            "         5.6514e-02, -2.3024e-02,  9.1788e-02,  1.8556e-01,  8.4253e-02,\n",
            "         3.2191e-01,  1.9675e-01,  1.2056e-01, -2.8450e-02,  1.0200e-01,\n",
            "         1.1421e-01,  1.2445e-01, -7.5833e-02,  7.0668e-02,  1.5295e-01,\n",
            "         9.8350e-02,  4.9804e-02, -2.0624e-02,  1.9799e-01,  1.0088e-01,\n",
            "         8.6509e-02,  3.6316e-03,  2.0526e-02,  3.3444e-02,  8.4986e-02,\n",
            "         3.4004e-01, -3.4023e-02,  1.6249e-01, -4.4230e-02,  5.5143e-02,\n",
            "        -8.3024e-02,  1.2218e-01,  2.8932e-01,  1.1383e-01,  2.7503e-01,\n",
            "         2.7413e-01,  7.3318e-02,  1.6877e-01,  4.7615e-02,  1.6114e-01,\n",
            "         5.9433e-02,  3.2650e-02,  9.4838e-02,  3.1401e-01, -2.1657e-01,\n",
            "        -4.0621e-02, -3.5819e-02,  1.0911e-01,  2.1233e-01,  1.1997e-01,\n",
            "         2.5731e-03,  1.0237e-01,  1.9480e-01,  2.0754e-01,  2.3612e-01,\n",
            "         3.3378e-02,  3.9276e-02,  2.5957e-02, -1.7888e-02,  1.3795e-01,\n",
            "        -2.7131e-02, -4.3515e-02,  7.9191e-02, -6.5444e-02,  1.0129e-01,\n",
            "        -1.6413e-01, -1.2533e-01, -2.1412e-01, -1.5993e-01,  5.9444e-03,\n",
            "         1.7359e-04, -1.0635e-02,  1.0286e-01, -1.2856e-01, -1.5458e-01,\n",
            "        -1.6868e-01, -7.7796e-02, -1.0969e-01,  5.7180e-02, -1.1895e-01,\n",
            "         1.0005e-01, -1.1579e-01, -2.6024e-02, -1.1702e-01,  1.1606e-02,\n",
            "        -2.3743e-03,  8.3136e-02, -2.1025e-01, -9.5982e-02, -2.0378e-01,\n",
            "        -2.5141e-02,  1.1258e-01,  3.5925e-02,  1.8520e-02, -4.6393e-02,\n",
            "         1.1467e-02, -1.0457e-01, -4.2248e-02,  1.1374e-01, -7.0638e-03,\n",
            "        -1.4161e-01, -7.9532e-02, -1.3239e-01,  4.8082e-02, -1.0422e-01,\n",
            "        -1.3754e-01, -1.8532e-01,  2.8754e-02, -8.6042e-02, -1.8181e-01,\n",
            "        -4.7994e-02,  7.6669e-02, -8.0304e-03, -2.0287e-01, -1.0304e-01,\n",
            "        -1.5901e-01, -4.5768e-03,  1.5080e-01, -1.8262e-01, -8.0950e-02,\n",
            "         8.8516e-02, -3.4272e-02, -2.6483e-02, -6.9052e-02, -6.0825e-03,\n",
            "        -1.1297e-01, -1.8395e-01, -2.1874e-01, -6.4370e-02,  4.5964e-03,\n",
            "        -8.1243e-02,  7.3698e-02,  5.2970e-02, -9.7346e-02, -1.8604e-01,\n",
            "        -1.5841e-01, -1.2289e-01,  1.3826e-01, -3.3715e-02, -1.3802e-01,\n",
            "        -1.3254e-01, -1.3725e-01, -1.5545e-01,  1.4605e-01,  4.9227e-02,\n",
            "         5.3315e-02,  1.5069e-02,  6.5076e-03, -6.6861e-02, -1.6313e-01,\n",
            "         1.1112e-01, -2.2208e-01, -1.2371e-01,  1.9899e-02, -1.1960e-01,\n",
            "         1.6435e-01, -2.0898e-02,  5.8793e-02, -3.2666e-02, -8.3597e-02,\n",
            "        -5.5779e-02,  1.2126e-02, -1.6003e-01, -5.0209e-02,  1.0666e-01,\n",
            "        -1.7149e-01,  7.3939e-02, -8.3436e-03,  6.5675e-02,  8.7250e-02,\n",
            "        -6.3064e-02,  1.7959e-02, -1.5334e-02, -1.8749e-01,  6.6486e-02,\n",
            "        -4.4912e-02, -1.0574e-01,  1.9973e-02, -1.8968e-01,  1.1365e-01,\n",
            "        -5.7198e-02, -5.8545e-02, -4.4256e-03,  1.2006e-01,  5.2621e-02,\n",
            "         1.3489e-01, -7.7514e-02, -1.3825e-01, -8.0780e-02,  2.0285e-02,\n",
            "         1.4160e-01, -6.7168e-02, -9.4997e-02,  6.7817e-02, -2.2585e-02,\n",
            "         1.8675e-02, -2.8267e-02,  3.0675e-03,  5.8418e-02,  9.9189e-03,\n",
            "         1.3172e-01,  8.9351e-02, -1.0370e-01, -3.3127e-02,  6.3271e-02,\n",
            "         2.8297e-02,  1.2472e-01, -1.2226e-01,  4.8720e-02,  6.7816e-02,\n",
            "        -8.2086e-02,  4.7914e-02, -2.0687e-02,  3.4941e-02,  1.1906e-01,\n",
            "         3.1306e-02,  7.1369e-02, -2.3219e-02, -2.2960e-02,  4.5288e-02,\n",
            "         2.3708e-02, -7.6781e-02,  1.0899e-01, -2.7446e-03, -3.6652e-02,\n",
            "         6.1848e-02,  1.2266e-01, -5.2874e-02, -1.4076e-02, -5.6515e-02,\n",
            "        -3.7807e-02, -7.1375e-02, -1.0882e-02, -1.9000e-02, -6.7244e-02,\n",
            "        -5.8061e-02,  9.6078e-02,  6.0569e-02,  6.1118e-03, -5.9364e-02,\n",
            "        -9.6722e-02,  1.4394e-02,  2.6838e-02,  5.1136e-02, -1.7584e-02,\n",
            "        -7.5552e-02,  1.9654e-02,  4.8241e-03,  1.0924e-01, -7.5948e-02,\n",
            "        -3.7997e-02, -1.9513e-02, -7.5297e-02,  2.9019e-02,  5.7413e-02,\n",
            "        -2.5052e-03, -7.4959e-02,  4.7951e-02, -4.0277e-02, -1.4855e-01,\n",
            "        -2.9343e-02, -5.6029e-02, -1.0016e-01, -1.2183e-02,  5.8481e-02,\n",
            "        -1.5821e-01,  6.7184e-02, -1.6542e-01, -3.4351e-02, -5.2414e-02,\n",
            "         1.0356e-01, -7.5132e-02,  7.9564e-02,  1.0725e-01,  6.6593e-02,\n",
            "         1.2059e-01,  1.2527e-01,  6.4667e-02, -1.5912e-01, -1.0556e-01,\n",
            "        -9.1794e-02,  4.3662e-03,  5.4125e-02, -7.2253e-02,  2.0869e-02,\n",
            "         4.2395e-02,  2.0837e-02, -9.8467e-02, -4.1870e-02,  1.2573e-01,\n",
            "        -1.1907e-01, -8.0517e-02, -1.4620e-01, -2.2026e-02,  1.0080e-01,\n",
            "        -1.1011e-01, -5.2701e-02,  1.0420e-01,  3.3030e-03, -6.9328e-02,\n",
            "         2.8948e-02, -4.0075e-02,  7.6035e-02, -8.2770e-02,  5.9567e-02,\n",
            "         1.2839e-01, -6.8391e-02, -1.1509e-01, -5.1113e-02,  6.0022e-02,\n",
            "        -8.3917e-02, -2.6163e-03,  8.4358e-02, -1.2851e-01, -1.1266e-02,\n",
            "        -3.7725e-03,  5.6733e-02,  1.1080e-01,  1.3264e-01, -5.2209e-03,\n",
            "         2.6714e-02, -2.7124e-01,  2.1157e-01,  1.8865e-01, -1.1712e-01,\n",
            "        -2.0362e-01,  4.3297e-02, -5.3711e-01, -3.2230e-01, -7.7197e-02,\n",
            "        -1.0853e-03,  1.2159e-01, -5.0124e-01,  4.9632e-02, -3.1857e-02,\n",
            "        -3.5267e-01, -4.1899e-01, -1.7165e-01,  2.2250e-01,  8.5634e-02,\n",
            "         5.6772e-02, -1.7994e-01,  2.9842e-02, -7.5664e-02,  1.0736e-01,\n",
            "        -4.4628e-01,  1.0622e-01,  4.2975e-02, -7.4218e-02, -5.3035e-02,\n",
            "         1.4485e-01, -1.9336e-02,  4.2758e-02,  8.1000e-02,  1.9985e-03,\n",
            "         2.8960e-02, -6.7636e-02,  2.0468e-01,  5.5490e-02,  8.3028e-02,\n",
            "        -4.8136e-02,  1.3493e-03,  1.5349e-02,  6.5622e-02, -7.3689e-02,\n",
            "        -8.6996e-03,  1.5573e-01,  1.8908e-02, -1.7767e-01,  7.4840e-02,\n",
            "        -4.2029e-01,  1.0211e-01, -2.4608e-02, -1.5967e-01,  2.9940e-02,\n",
            "         8.2276e-02,  5.1428e-02,  3.2577e-02, -1.4096e-01, -1.2028e-01,\n",
            "        -1.6432e-01,  9.7903e-02, -1.3829e-01,  1.3257e-01, -2.3850e-01,\n",
            "        -5.5431e-01, -2.1829e-01, -6.1498e-02, -3.2388e-01,  1.0686e-01,\n",
            "         8.4661e-02,  6.5342e-02,  7.0844e-02, -5.4283e-02, -1.1630e-01,\n",
            "        -4.5306e-02, -2.4224e-02, -1.0803e-01,  1.4590e-01, -1.2440e-01,\n",
            "         2.1789e-02, -5.2065e-01, -1.0837e-01, -1.0981e-01, -1.3323e-01,\n",
            "        -9.8269e-02, -1.0518e-01,  5.5890e-03,  1.3323e-01,  1.3900e-02,\n",
            "        -1.2074e-01,  1.8635e-02,  7.4523e-02, -2.1964e-02,  3.1856e-01,\n",
            "        -4.5602e-02,  6.7576e-02, -9.4989e-03,  1.2338e-01,  8.7879e-02,\n",
            "         5.3463e-02, -5.8534e-02, -4.7193e-01, -2.8192e-02, -4.4089e-01,\n",
            "         1.5524e-01,  4.0712e-03,  6.7610e-02,  1.1643e-01, -4.3795e-01,\n",
            "        -2.5088e-01,  2.6906e-02,  4.0609e-04, -5.0226e-02,  2.2214e-02,\n",
            "        -1.3682e-01, -1.1771e-01,  2.9444e-01,  1.1945e-02, -2.1152e-01,\n",
            "        -3.8530e-01,  1.4142e-01,  6.4114e-02,  3.0370e-02, -1.0737e-01,\n",
            "        -1.7164e-01,  6.7393e-02], device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.bias_hh_l0 Parameter containing:\n",
            "tensor([ 2.1448e-01,  2.6563e-01, -8.2303e-03, -2.2182e-02,  1.8798e-01,\n",
            "        -2.1765e-02,  2.4063e-02,  2.2053e-01,  9.0432e-02,  7.5111e-03,\n",
            "         1.0986e-01,  1.5150e-01,  9.9534e-02,  6.4975e-02,  1.6881e-01,\n",
            "         2.2579e-01, -5.9272e-02,  9.9394e-02,  1.9028e-01,  1.5348e-01,\n",
            "         4.3435e-01,  2.2518e-01, -1.2509e-02,  1.6202e-01,  1.2339e-01,\n",
            "         1.5244e-01,  2.3735e-02,  2.5607e-01,  2.3746e-01,  2.0556e-01,\n",
            "         1.8350e-01,  2.5648e-01,  1.7270e-01,  1.0899e-01,  2.2192e-01,\n",
            "         1.2267e-01, -5.0159e-02,  1.3207e-01,  1.1685e-01,  2.2950e-02,\n",
            "         6.1469e-02,  1.4740e-01,  1.8683e-01,  8.2811e-02,  2.1161e-01,\n",
            "         2.2978e-01,  6.7709e-02,  6.6394e-02,  2.4977e-01,  2.1223e-01,\n",
            "         2.1596e-01, -5.8200e-02,  1.3815e-01,  3.0322e-01,  8.0018e-03,\n",
            "        -1.6879e-01,  2.3072e-01,  5.8019e-01, -1.2794e-02,  4.6545e-02,\n",
            "         7.8736e-02,  1.6229e-01,  5.1919e-02, -4.2716e-02,  5.1288e-05,\n",
            "         1.5935e-01,  2.6752e-02,  9.1806e-02,  9.7644e-02, -2.8377e-02,\n",
            "         2.6699e-01,  1.7394e-01,  4.7547e-02, -4.5801e-02,  1.8333e-01,\n",
            "         1.6220e-01, -2.5671e-03, -2.1322e-02,  8.8722e-02,  1.1228e-01,\n",
            "         1.1023e-01,  2.7453e-02,  4.1635e-02,  1.4881e-01,  4.4361e-03,\n",
            "         1.1669e-01, -1.0887e-01,  1.3803e-01,  4.2117e-02,  1.0720e-02,\n",
            "         2.2336e-01, -6.6722e-02,  1.3935e-01, -2.0514e-02,  9.5031e-02,\n",
            "        -3.5834e-02,  9.1488e-02,  1.9648e-01,  2.7131e-01,  2.9360e-01,\n",
            "         2.7696e-01,  1.2043e-01,  2.3400e-01,  6.6473e-02,  9.3015e-02,\n",
            "        -9.3433e-03,  4.6801e-02,  1.9172e-01,  4.0433e-01, -2.3760e-01,\n",
            "        -4.1285e-03,  4.7885e-02,  1.4424e-01,  1.4300e-01,  1.1671e-01,\n",
            "         3.6467e-02,  2.2567e-02,  1.6768e-01,  6.6912e-02,  1.7862e-01,\n",
            "         3.7095e-02,  5.0748e-02,  1.6905e-01,  1.0950e-01,  1.6180e-01,\n",
            "         1.0244e-01,  1.2233e-03,  8.1032e-02, -4.3050e-02, -3.2561e-03,\n",
            "        -3.4617e-02, -1.5047e-01, -1.4282e-01, -1.5499e-01,  1.1526e-03,\n",
            "        -5.1700e-02,  6.4642e-02, -9.7270e-03, -1.1337e-01, -2.2087e-01,\n",
            "        -6.8840e-02, -5.4946e-03, -1.7013e-01, -1.2466e-02, -1.3217e-01,\n",
            "        -2.5983e-02, -1.1619e-01, -4.2332e-02,  3.6436e-02,  6.7801e-02,\n",
            "         1.0005e-01,  1.7636e-01, -6.5557e-02,  3.1206e-02, -6.4503e-02,\n",
            "        -5.1444e-02,  1.3893e-02,  4.0469e-02, -2.2186e-02, -1.0097e-01,\n",
            "         4.4513e-02, -7.3634e-02,  1.1995e-02,  6.6931e-02, -4.1728e-02,\n",
            "        -1.5169e-01, -1.1256e-01, -2.0409e-01,  5.9999e-02, -1.3897e-01,\n",
            "        -9.2309e-02, -2.2524e-01,  6.2076e-02, -1.2674e-01, -1.4872e-01,\n",
            "        -1.9128e-01,  3.4015e-02,  3.4034e-02, -4.4028e-02, -1.8618e-02,\n",
            "        -4.3251e-05, -5.9355e-02,  1.7800e-01, -7.8231e-02, -1.7915e-02,\n",
            "         8.5366e-02, -9.4212e-02, -3.4124e-03, -1.6340e-01, -5.3951e-02,\n",
            "        -6.1165e-02, -9.4769e-02, -2.0859e-01, -2.8177e-02, -2.7700e-02,\n",
            "        -1.4358e-01,  1.6852e-01, -5.6643e-02, -1.4101e-01, -2.3415e-01,\n",
            "        -1.1607e-01, -1.7316e-01,  1.7601e-01, -1.4477e-01, -2.3088e-01,\n",
            "        -2.0849e-01, -1.4985e-01, -5.4417e-04,  8.9237e-02, -2.0971e-03,\n",
            "         6.6431e-02,  8.6246e-02,  2.8902e-02,  5.4988e-02, -1.3993e-01,\n",
            "         1.4633e-01, -1.0470e-01, -2.0167e-01, -2.8382e-02, -1.6712e-01,\n",
            "         5.5723e-02, -5.0054e-02, -7.0923e-02, -1.3507e-01, -1.8035e-01,\n",
            "        -1.3450e-02, -5.5565e-02, -2.2301e-01, -1.1225e-01,  1.5428e-01,\n",
            "        -1.0005e-01,  6.5807e-02,  1.1402e-01,  5.0107e-02,  9.7022e-02,\n",
            "        -8.6861e-02,  5.0832e-02, -1.6047e-01, -2.0489e-01,  4.9967e-02,\n",
            "        -9.8294e-02, -6.6168e-03, -2.8208e-02, -1.4541e-01,  1.1670e-01,\n",
            "         6.9566e-02, -1.7975e-01,  2.3966e-02,  1.2424e-02,  1.4091e-02,\n",
            "         1.3253e-01, -1.5602e-01, -1.3521e-01, -8.3625e-02, -4.0638e-02,\n",
            "         8.9269e-02, -5.3923e-02, -1.0238e-01,  6.6824e-02,  8.2414e-02,\n",
            "        -6.1882e-02,  9.5404e-02, -1.0574e-01,  2.1616e-02, -4.7748e-03,\n",
            "         1.1878e-01, -3.6036e-03,  1.3136e-02,  6.1602e-02,  5.2771e-02,\n",
            "        -4.1450e-03, -2.7996e-02, -1.1028e-01,  2.6956e-02,  7.1064e-02,\n",
            "        -5.5820e-03, -2.3584e-02, -1.3412e-01,  2.0150e-02,  4.2441e-02,\n",
            "         4.3076e-02,  3.6658e-02,  7.3455e-02,  3.0903e-02,  3.3865e-02,\n",
            "         7.3782e-02, -4.2483e-02, -4.3205e-02,  8.7518e-02, -4.4123e-02,\n",
            "        -2.1372e-02,  1.0090e-01, -3.8493e-02, -3.9857e-02,  4.9117e-02,\n",
            "        -1.6661e-02, -9.5647e-02,  8.2414e-02,  7.8853e-02,  4.5732e-02,\n",
            "         1.0611e-01,  4.7889e-03, -6.1419e-02,  5.5167e-02,  3.4770e-02,\n",
            "        -7.1420e-02, -1.8494e-02, -5.7468e-02,  5.1875e-02, -6.3965e-02,\n",
            "        -8.2455e-03,  7.1570e-02,  8.4949e-02,  1.1954e-01, -5.9271e-02,\n",
            "        -5.9207e-02, -9.3704e-02,  2.6644e-02,  1.3011e-02, -1.2284e-02,\n",
            "        -1.2944e-01, -4.2030e-03,  8.3916e-03, -8.7425e-02, -1.6572e-02,\n",
            "        -5.2862e-03, -6.3662e-02, -1.7703e-02, -6.2695e-02,  8.2646e-02,\n",
            "        -1.6536e-02, -9.3420e-03, -2.4440e-02,  8.6569e-02,  9.5716e-02,\n",
            "         2.7244e-02, -1.4819e-01, -3.4290e-05,  6.7832e-02,  1.4172e-01,\n",
            "        -7.2903e-03,  1.1368e-01,  8.1303e-02, -1.4672e-01,  6.0783e-03,\n",
            "         7.4624e-03,  1.0823e-02,  5.3679e-02, -1.2163e-01,  9.7845e-02,\n",
            "         1.0710e-01,  9.8911e-02, -1.0519e-01, -8.4324e-02,  1.0532e-01,\n",
            "        -1.3689e-03, -2.6721e-02, -1.4761e-01,  1.1324e-01,  2.4062e-02,\n",
            "        -2.6038e-02,  5.4100e-02,  1.4301e-01, -3.7615e-02, -4.8015e-02,\n",
            "         1.4490e-02, -3.1923e-02,  2.4268e-03, -1.2634e-02,  7.7476e-02,\n",
            "         1.2025e-01,  3.7695e-02, -1.2154e-01, -6.7983e-02, -1.6910e-02,\n",
            "         6.8292e-03,  4.3806e-02,  3.8573e-02, -1.3240e-01, -3.7531e-02,\n",
            "        -9.2948e-02, -6.0028e-02,  3.6568e-02,  1.3006e-01, -1.1154e-01,\n",
            "         4.0988e-02, -3.4811e-01,  1.1972e-01,  7.0767e-02, -2.2628e-02,\n",
            "        -1.4484e-01, -3.1240e-02, -4.2631e-01, -1.8398e-01,  5.2828e-02,\n",
            "        -3.6565e-02,  9.1425e-02, -5.4954e-01,  4.3399e-02,  3.9758e-03,\n",
            "        -3.9866e-01, -4.4814e-01, -1.4595e-01,  1.5505e-01,  1.0245e-01,\n",
            "         1.1864e-01, -2.8243e-01, -6.3799e-02, -1.2485e-01,  1.1771e-01,\n",
            "        -3.9515e-01,  6.5164e-02,  5.3423e-02, -6.9509e-02,  3.9536e-02,\n",
            "         4.8456e-02, -1.0172e-01,  1.6663e-01,  4.7668e-02, -1.1174e-01,\n",
            "         2.4312e-02, -6.6531e-02,  9.8232e-02, -1.6506e-02,  1.2617e-02,\n",
            "        -1.1383e-02,  2.0718e-02,  1.9537e-03,  1.2723e-01, -4.5282e-02,\n",
            "         3.4384e-02,  1.8867e-01,  1.2752e-01, -1.2677e-01,  2.7571e-02,\n",
            "        -4.9697e-01,  2.0288e-01, -4.8301e-02, -1.5479e-01,  6.1726e-02,\n",
            "         1.4110e-01, -2.7139e-02, -1.2399e-02, -1.7666e-01, -1.6978e-01,\n",
            "        -1.4917e-01,  1.4758e-01, -2.0976e-02,  2.1500e-02, -1.9314e-01,\n",
            "        -5.0463e-01, -1.2901e-01, -1.0700e-01, -4.3330e-01,  1.4962e-01,\n",
            "         7.1977e-02,  1.3454e-01, -6.9035e-04, -2.6572e-02, -6.4018e-02,\n",
            "        -2.8989e-02, -9.5375e-03,  4.7881e-02,  1.6021e-01, -5.8531e-02,\n",
            "        -2.6133e-02, -4.8074e-01, -3.6754e-02, -1.3074e-01, -2.8566e-02,\n",
            "        -1.1747e-01, -4.9963e-02, -5.1382e-03,  1.3857e-01,  1.8733e-02,\n",
            "        -1.3466e-01, -6.3425e-03, -2.7585e-02, -7.1687e-02,  3.8165e-01,\n",
            "         2.1791e-04,  2.6348e-02,  3.1969e-03,  3.2451e-03,  1.3719e-01,\n",
            "        -8.0286e-02, -5.6122e-02, -5.2290e-01,  6.4458e-02, -5.6540e-01,\n",
            "         8.8457e-02, -4.2703e-02,  1.5084e-03,  1.2755e-01, -4.4180e-01,\n",
            "        -1.6620e-01, -4.9269e-02,  1.3464e-01, -6.3656e-02,  1.1219e-01,\n",
            "        -1.0946e-01, -8.9583e-02,  1.7340e-01,  6.4138e-02, -7.2230e-02,\n",
            "        -2.7218e-01,  5.4931e-02,  1.1112e-01,  8.5222e-03, -8.1351e-02,\n",
            "        -1.9005e-01,  8.1364e-03], device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.weight_ih_l1 Parameter containing:\n",
            "tensor([[-0.0833, -0.0432,  0.0146,  ...,  0.0527,  0.0177,  0.0823],\n",
            "        [-0.0522, -0.1301, -0.0757,  ..., -0.0928,  0.0920,  0.0519],\n",
            "        [-0.1235, -0.0881, -0.1022,  ...,  0.0829, -0.0679,  0.0966],\n",
            "        ...,\n",
            "        [-0.1114,  0.1421,  0.0749,  ..., -0.1584, -0.1922, -0.0611],\n",
            "        [ 0.0411,  0.4586, -0.1940,  ..., -1.0004,  0.7056, -0.6653],\n",
            "        [-0.2552,  0.1517, -0.6298,  ..., -0.3580,  0.8220, -0.0645]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.weight_hh_l1 Parameter containing:\n",
            "tensor([[ 0.0788, -0.0624,  0.1745,  ...,  0.0119, -0.0520,  0.0943],\n",
            "        [-0.0276,  0.0047,  0.0147,  ...,  0.0413,  0.0052,  0.0480],\n",
            "        [ 0.0022, -0.0975,  0.0736,  ..., -0.0071,  0.0486,  0.0518],\n",
            "        ...,\n",
            "        [-0.1949, -0.0042, -0.0697,  ..., -0.2078,  0.3621,  0.1055],\n",
            "        [-0.2052,  0.1685,  0.0804,  ...,  0.1142,  0.0702, -0.0422],\n",
            "        [ 0.0579, -0.0886,  0.0892,  ..., -0.3519, -0.0628, -0.1040]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.bias_ih_l1 Parameter containing:\n",
            "tensor([-5.1426e-02, -6.1431e-02,  3.7312e-02,  9.3117e-02, -8.5108e-02,\n",
            "         7.4948e-02, -1.9619e-02,  1.1561e-01,  1.0429e-01,  7.5125e-03,\n",
            "         5.8406e-02,  1.2746e-02,  3.7199e-02,  1.0725e-01,  5.3198e-02,\n",
            "         1.8074e-02,  9.7978e-02,  8.9592e-02,  2.4200e-02,  1.1689e-01,\n",
            "         7.0460e-02,  1.1178e-01, -4.4007e-02,  5.5285e-02, -1.8825e-03,\n",
            "         2.5118e-03, -2.5842e-02,  3.4549e-02, -3.5413e-02, -8.8939e-02,\n",
            "        -3.6293e-02,  4.0098e-02, -8.2178e-02,  1.0705e-01,  1.3020e-01,\n",
            "         2.3222e-02,  9.3907e-02,  6.1693e-02, -1.3703e-01, -5.4153e-02,\n",
            "        -9.4289e-02, -1.1336e-02, -3.9059e-02, -1.2296e-02, -4.8464e-02,\n",
            "        -2.3990e-02,  8.2007e-02,  1.9108e-02, -9.2358e-04,  1.2434e-01,\n",
            "        -1.8807e-02, -1.0160e-02,  1.6502e-02,  1.0877e-01, -7.3068e-02,\n",
            "         5.0176e-02,  1.3645e-04,  3.5653e-02,  9.1721e-02, -6.2777e-02,\n",
            "         2.3725e-02,  1.0293e-01,  1.7423e-02,  9.4365e-02,  1.0440e-01,\n",
            "         1.5678e-01,  1.3632e-02, -1.3559e-02, -8.3940e-02,  7.1493e-02,\n",
            "         6.1730e-03, -2.0239e-02, -3.5475e-02,  2.8359e-02, -1.0272e-01,\n",
            "        -4.4782e-02,  1.2722e-01,  9.0218e-02,  1.9736e-02,  6.1959e-02,\n",
            "         6.6258e-02,  9.7557e-02, -5.0833e-02,  6.3323e-02,  3.9657e-02,\n",
            "        -2.0604e-02,  6.5567e-02,  1.2278e-01, -2.5679e-02, -4.3953e-02,\n",
            "         1.5945e-02,  6.4277e-02,  4.9595e-02, -3.2957e-02,  7.7344e-02,\n",
            "        -6.9386e-03,  7.7955e-02, -6.0473e-02, -8.7700e-02, -4.1922e-02,\n",
            "         8.0699e-02,  1.1219e-01, -4.5243e-02, -7.2288e-02, -2.8932e-02,\n",
            "         1.8519e-01, -2.0717e-02, -5.0430e-02, -2.5895e-02,  4.5109e-02,\n",
            "         4.7551e-02, -2.1947e-02,  1.9919e-02,  8.4708e-02, -2.2365e-03,\n",
            "         6.6866e-02, -5.9747e-03,  1.1421e-02,  2.7797e-02,  6.8593e-02,\n",
            "         5.1338e-02,  5.1540e-02,  5.0243e-02,  1.1699e-01, -1.0057e-02,\n",
            "         2.8920e-02, -2.9593e-02,  6.5367e-02,  9.8759e-02,  1.0904e-01,\n",
            "         9.4922e-02,  1.3876e-01, -4.2808e-02, -1.7143e-03,  6.5700e-02,\n",
            "         4.7531e-02, -1.2721e-02,  4.3794e-02,  1.3271e-01,  1.1070e-01,\n",
            "         1.3042e-03,  1.5732e-02,  7.3130e-02,  4.5821e-02,  7.9895e-02,\n",
            "         4.5646e-02,  9.9925e-02,  7.0734e-02, -2.6073e-02,  6.3363e-02,\n",
            "        -4.4505e-03,  1.1823e-01,  3.3530e-02, -2.8940e-02,  3.6908e-02,\n",
            "         1.4672e-01,  5.5088e-02,  1.6079e-02, -2.2939e-02, -4.9488e-02,\n",
            "        -3.6509e-02,  2.2244e-02,  6.5450e-02,  3.5849e-02,  1.9302e-02,\n",
            "         4.8007e-02, -4.3623e-02,  1.5490e-02, -3.2442e-03,  7.5216e-02,\n",
            "         1.0659e-01,  1.6729e-02, -6.0425e-02,  7.7849e-02,  9.0313e-02,\n",
            "        -2.4058e-02, -1.7633e-02,  1.1964e-01, -4.2599e-02, -1.3458e-03,\n",
            "         2.5385e-03, -6.4244e-02, -2.2682e-02, -4.2422e-02, -5.8301e-02,\n",
            "         5.0437e-02, -4.7653e-02, -3.5072e-04, -5.9172e-02, -2.4602e-02,\n",
            "        -8.1786e-03,  3.5636e-03, -3.1879e-03, -6.3627e-02,  5.5529e-02,\n",
            "         6.0300e-03, -2.9801e-02,  7.6933e-02,  3.1894e-02, -8.1045e-02,\n",
            "         1.3346e-01,  1.1664e-01, -1.2730e-02,  5.1462e-02,  2.3340e-02,\n",
            "         1.3778e-01,  1.6768e-02,  9.1366e-02,  3.1632e-02,  6.4154e-02,\n",
            "         1.9609e-02,  2.0634e-02, -3.2830e-02,  8.6637e-02,  1.2712e-01,\n",
            "         9.1108e-02,  1.2548e-01,  1.2744e-01, -2.4056e-02,  1.1064e-01,\n",
            "         1.1605e-02, -1.0352e-02,  1.0244e-01,  4.4135e-02,  5.5573e-02,\n",
            "         1.2287e-01,  1.5770e-01,  5.8956e-02, -3.3799e-02,  1.2138e-01,\n",
            "         2.9266e-02,  6.2550e-02,  4.8307e-02,  5.8363e-02,  7.8013e-02,\n",
            "         1.0056e-01, -3.3340e-02,  7.2091e-03,  1.0065e-01, -6.4518e-02,\n",
            "         9.5934e-02,  1.2521e-01, -7.4788e-03,  8.0533e-02,  3.8010e-02,\n",
            "         2.3636e-02,  1.3142e-01,  5.3603e-04,  2.1333e-03, -2.4549e-02,\n",
            "         8.4535e-02, -2.4181e-02, -5.4169e-03,  9.1466e-02, -4.0899e-02,\n",
            "         1.6918e-02,  7.4714e-02, -5.5286e-02,  9.5763e-02,  8.1223e-02,\n",
            "         8.1926e-02, -7.2124e-03, -1.1986e-01, -6.7677e-03, -5.3570e-02,\n",
            "        -1.1723e-01, -5.6156e-03,  3.8814e-04, -5.8392e-04,  5.4238e-02,\n",
            "         1.9195e-02,  1.1278e-02, -3.8546e-02,  4.5054e-02, -5.4495e-02,\n",
            "        -3.7943e-02,  9.5536e-02, -7.6375e-02, -5.8641e-03,  1.1340e-01,\n",
            "         1.0360e-01, -1.5540e-02,  4.8633e-02, -8.6787e-03, -9.7744e-02,\n",
            "         8.6028e-02,  4.7146e-02, -1.3893e-02,  8.2414e-02, -8.0893e-02,\n",
            "        -3.0896e-02, -6.8919e-02,  6.6448e-02, -1.2570e-01, -7.8802e-02,\n",
            "        -6.1695e-02,  2.9074e-02, -2.1631e-02, -8.3480e-02,  1.6173e-02,\n",
            "        -4.9526e-02,  3.2745e-02, -2.4766e-02, -1.3732e-02,  9.5102e-02,\n",
            "        -4.9817e-02, -8.9100e-02, -1.0338e-01,  3.9273e-02,  2.1955e-02,\n",
            "         3.6277e-02,  6.3479e-02, -4.9259e-02,  1.0969e-01, -2.6850e-02,\n",
            "         3.7862e-02, -4.2948e-02,  1.3271e-02,  3.3476e-03, -8.9763e-03,\n",
            "        -4.9264e-02, -4.8481e-02, -6.8061e-02, -9.2581e-03, -1.7468e-03,\n",
            "         1.8491e-02, -3.0682e-02,  1.7527e-02,  1.2130e-01, -2.6871e-02,\n",
            "         3.7333e-02,  3.8342e-02,  2.5374e-02,  3.4423e-02,  1.7148e-03,\n",
            "         2.6001e-02, -1.0307e-01,  1.1766e-01, -8.3280e-02,  9.6237e-02,\n",
            "        -5.5281e-02,  1.0207e-01,  6.4264e-02,  1.0154e-01,  1.3264e-01,\n",
            "        -7.6690e-02, -1.0203e-01, -5.3433e-02,  2.9796e-02, -6.4412e-02,\n",
            "         9.2588e-02,  1.1308e-01, -1.1410e-01, -7.4294e-02,  4.1009e-03,\n",
            "        -6.4938e-02,  1.7020e-02, -4.1298e-02,  1.1681e-01, -1.2954e-01,\n",
            "         7.5314e-02,  4.8900e-02, -1.8396e-02, -1.1885e-01,  8.7308e-02,\n",
            "         8.0154e-02, -8.6009e-02, -1.1418e-01, -3.1181e-02, -9.9946e-02,\n",
            "        -7.7558e-02, -8.5170e-02, -6.1747e-02, -1.7242e-02, -1.0675e-01,\n",
            "        -7.7327e-02, -1.0511e-01, -7.6852e-02, -2.6633e-02,  1.0066e-01,\n",
            "        -7.1372e-02, -1.2628e-01,  1.5641e-02, -9.5772e-02, -3.9306e-02,\n",
            "        -2.9991e-02, -2.3177e-02, -5.7871e-03,  1.0829e-01, -8.4687e-02,\n",
            "        -5.6862e-03, -9.0714e-02,  4.8931e-02, -7.4772e-02,  2.2073e-01,\n",
            "         5.5759e-02, -6.1803e-02,  5.5198e-02, -1.3488e-01, -6.4151e-02,\n",
            "         5.3399e-02,  1.2753e-02, -9.0461e-02, -3.7181e-02, -4.2894e-02,\n",
            "        -6.1127e-02,  1.6646e-03, -1.6054e-01,  8.8730e-02, -1.3503e-03,\n",
            "        -4.0229e-02,  3.9987e-01, -3.2995e-02,  1.7445e-01,  1.0724e-03,\n",
            "         6.7657e-02,  1.8804e-02, -1.2874e-01, -4.3943e-02, -2.1178e-01,\n",
            "         5.0574e-02, -4.9567e-02,  1.6401e-01, -2.3403e-02,  9.6085e-02,\n",
            "        -5.2363e-02, -1.1938e-01,  3.5031e-01, -8.7922e-02, -9.0865e-02,\n",
            "        -3.0201e-02, -4.6232e-02,  2.5834e-01, -7.6697e-02, -7.5110e-02,\n",
            "        -6.0156e-02,  1.4132e-01,  7.9873e-02,  3.3006e-02,  2.4034e-02,\n",
            "        -4.6328e-02, -9.3676e-02, -1.0186e-01,  1.1536e-01, -2.3130e-02,\n",
            "         9.4841e-02, -3.3321e-02, -1.1736e-01,  5.8106e-02,  6.5370e-02,\n",
            "         1.1433e-01,  2.3421e-01,  4.2488e-01, -7.4071e-02, -8.2845e-04,\n",
            "        -5.1558e-02, -2.3151e-02,  3.5922e-02,  1.2758e-02,  9.4758e-05,\n",
            "        -1.4749e-01, -3.7895e-02, -1.8239e-01,  3.6064e-03, -1.0341e-01,\n",
            "        -4.8489e-04, -1.4242e-02, -3.2096e-02,  2.1004e-02,  9.5203e-02,\n",
            "        -6.5201e-02,  4.1957e-02, -4.1395e-02, -1.2293e-01,  6.6048e-03,\n",
            "         2.1338e-02, -1.1188e-01,  5.0036e-02, -7.0640e-02,  1.7441e-02,\n",
            "         2.1853e-02, -6.1524e-02,  3.7488e-01,  1.6156e-01, -6.9118e-02,\n",
            "        -6.8129e-02, -3.1634e-03,  7.2509e-02, -6.6157e-02,  4.3322e-03,\n",
            "         3.0967e-01, -6.2103e-02,  3.3861e-01, -1.0410e-03, -1.2542e-01,\n",
            "        -9.4423e-02,  7.1670e-02, -5.0715e-02, -2.0874e-02,  2.4394e-02,\n",
            "         9.8081e-02, -1.1908e-01, -4.5422e-02, -1.7101e-01, -1.6534e-02,\n",
            "        -1.0144e-02, -1.0438e-01,  1.0334e-02, -5.5224e-02, -3.5922e-02,\n",
            "        -1.5240e-01,  2.1211e-01], device='cuda:0', requires_grad=True)\n",
            "decoder.rnn.bias_hh_l1 Parameter containing:\n",
            "tensor([ 3.7921e-02, -6.6126e-02, -3.6066e-02,  6.6371e-03, -2.6728e-02,\n",
            "        -4.3204e-02,  5.0767e-02, -5.2799e-02,  1.0011e-01,  9.9024e-02,\n",
            "         2.8129e-02, -4.4035e-02,  1.2728e-01,  5.9174e-02,  7.3981e-02,\n",
            "         7.8819e-02,  9.4077e-02,  6.8946e-02,  3.7119e-02, -5.4388e-02,\n",
            "         4.6575e-03,  4.4828e-02, -1.1284e-02, -4.1362e-02,  6.6707e-02,\n",
            "         2.5983e-02,  1.0714e-02, -4.5478e-02,  6.7266e-02, -6.0840e-02,\n",
            "         3.1070e-02,  1.0274e-02, -3.0231e-03,  7.6590e-02,  1.2240e-01,\n",
            "        -6.5122e-02,  7.3562e-02,  1.6388e-01, -5.5470e-02, -5.7956e-02,\n",
            "        -5.9414e-02, -5.5856e-02,  8.3502e-02,  1.2947e-01,  1.1205e-01,\n",
            "        -5.6768e-02,  9.3004e-02,  1.0033e-01, -1.4057e-01,  1.0102e-01,\n",
            "        -3.2151e-02,  7.8763e-02, -9.4585e-04,  1.0682e-01, -8.6293e-02,\n",
            "        -1.1915e-02,  3.2040e-02,  4.2941e-02,  1.1781e-01,  5.7253e-02,\n",
            "         8.2001e-02,  3.5643e-02, -6.0227e-02,  1.2622e-01,  3.0410e-02,\n",
            "         7.5527e-02, -5.8566e-02,  2.3823e-02,  5.3996e-02,  2.0347e-02,\n",
            "         6.4676e-02,  7.6912e-02, -4.3544e-02,  5.5134e-02,  3.3371e-02,\n",
            "         6.8638e-02,  2.5894e-02,  1.1683e-01,  1.1441e-01, -4.2654e-02,\n",
            "         1.3012e-01,  2.3224e-02,  1.6606e-02,  1.1462e-01,  8.5586e-02,\n",
            "        -1.3846e-02,  1.0806e-01,  2.2962e-03,  4.4884e-02,  2.3827e-02,\n",
            "         4.5484e-02,  9.6009e-02,  1.3344e-01, -1.2093e-02,  1.0834e-01,\n",
            "         7.0261e-02,  3.9998e-02, -3.2266e-02, -1.2204e-01, -1.1082e-01,\n",
            "         6.4908e-02,  5.5678e-02, -2.9236e-02, -6.7697e-02,  3.6597e-02,\n",
            "         1.7291e-01,  5.7432e-02,  4.5598e-02,  9.5916e-02,  2.9375e-02,\n",
            "         1.1021e-01,  1.0866e-02, -4.7126e-02,  9.2100e-02,  2.3646e-02,\n",
            "         1.1027e-02,  6.1476e-02, -7.1985e-02, -4.6510e-02,  1.0235e-01,\n",
            "        -1.6832e-02, -1.3898e-02, -2.8322e-02,  8.2410e-02,  1.0702e-01,\n",
            "        -6.3285e-03, -8.8430e-03, -6.8643e-02,  7.2973e-02, -3.4766e-02,\n",
            "         6.7017e-02,  2.1762e-02,  3.8880e-03,  1.1042e-01,  2.6193e-03,\n",
            "         1.0565e-01,  5.7234e-02,  7.9820e-03,  7.8896e-02, -5.0687e-02,\n",
            "        -6.8082e-02,  4.6836e-02,  5.5564e-02,  5.4529e-02,  1.8241e-02,\n",
            "        -4.0380e-02,  9.0063e-02,  1.0088e-01,  5.7550e-02,  7.9823e-02,\n",
            "        -1.6641e-02,  1.1699e-01,  1.6632e-02, -4.2104e-02,  1.4738e-01,\n",
            "        -1.8439e-02,  1.6967e-02,  2.5769e-02,  4.8220e-02, -7.4646e-02,\n",
            "         9.6172e-02,  4.7465e-02,  8.1167e-02, -1.3071e-02,  2.8631e-02,\n",
            "         2.5352e-04,  4.2763e-02,  8.5945e-02,  6.2907e-02,  2.8546e-02,\n",
            "         1.0366e-01, -2.0707e-02,  7.1688e-03,  7.8405e-02, -3.3024e-02,\n",
            "         2.7999e-02,  5.0606e-02,  4.5579e-04,  7.3655e-02, -2.1289e-02,\n",
            "         5.2805e-02, -4.5190e-02,  7.1004e-02,  3.5389e-02,  5.9112e-02,\n",
            "        -1.2458e-02,  1.6494e-02,  6.1358e-02, -4.1490e-02, -5.6758e-02,\n",
            "         8.9007e-02,  1.2458e-01, -8.3289e-02,  4.3484e-02, -2.3848e-02,\n",
            "        -3.5901e-02,  3.2519e-02,  2.2161e-02,  1.2084e-01, -3.7726e-02,\n",
            "         1.2375e-01,  4.8084e-02,  8.1017e-03,  9.9761e-02, -1.6752e-02,\n",
            "         6.0032e-04,  2.4660e-02,  7.3695e-02,  6.9391e-02,  8.1704e-02,\n",
            "         1.2106e-01,  7.4354e-02,  1.2282e-01, -4.2986e-02,  2.0041e-02,\n",
            "        -3.1940e-02, -4.5847e-02,  7.8179e-02,  6.7317e-02,  2.3066e-02,\n",
            "        -4.0510e-04,  2.6368e-02, -6.3029e-04,  2.6877e-02,  8.0188e-02,\n",
            "         9.5352e-02,  1.6724e-01,  6.8373e-02,  1.1626e-01, -6.1218e-03,\n",
            "        -2.1316e-02, -1.6046e-02,  1.2764e-01,  1.2077e-02,  1.1389e-01,\n",
            "         4.4896e-02, -2.9081e-02,  1.1530e-01,  2.8029e-02,  2.4373e-02,\n",
            "        -5.8865e-03,  4.1783e-02,  4.6773e-02,  5.8620e-02,  5.6897e-02,\n",
            "         2.1571e-02,  1.4357e-01,  8.3133e-02,  1.2850e-01,  9.1675e-02,\n",
            "         1.3180e-01,  1.2371e-01, -2.1517e-02,  2.0474e-02,  9.5220e-02,\n",
            "         1.0815e-04,  9.4971e-02, -9.3019e-02,  6.0549e-02,  8.3190e-02,\n",
            "         1.1913e-01,  1.0567e-01, -1.1606e-01,  6.6688e-02,  5.0002e-02,\n",
            "         4.5577e-02,  4.4938e-03,  1.0192e-01, -2.1204e-02, -3.3787e-02,\n",
            "        -1.1966e-01,  6.7599e-02,  2.7038e-02,  5.9617e-02, -6.6271e-03,\n",
            "         9.5713e-02,  1.0729e-01, -7.8769e-02,  7.1650e-02, -7.1962e-03,\n",
            "         1.4142e-02,  1.1489e-01,  4.5610e-02, -8.3566e-02, -9.8449e-02,\n",
            "         2.5629e-02,  7.6877e-02, -1.1040e-01,  1.1972e-01, -5.8955e-02,\n",
            "        -1.1031e-01, -2.8449e-02,  1.0165e-01, -6.3286e-02, -6.2674e-03,\n",
            "        -5.0821e-02,  5.1078e-02, -1.0105e-01, -1.5705e-02,  1.3506e-03,\n",
            "        -9.7960e-02,  8.1641e-02, -9.9103e-02, -3.7324e-02, -7.5944e-03,\n",
            "        -8.6130e-02, -9.5712e-02, -1.4119e-02,  8.7702e-02,  8.2565e-02,\n",
            "        -2.1682e-02,  6.2453e-02, -9.6648e-02,  5.7380e-02, -1.0008e-01,\n",
            "         8.6203e-02, -7.1627e-02, -2.0526e-02, -8.6786e-02,  9.1653e-02,\n",
            "         5.7170e-03,  2.4828e-02, -1.0074e-01, -1.8274e-02, -8.9856e-02,\n",
            "         7.4999e-02, -9.0407e-02, -5.3793e-02,  9.2129e-02, -3.0003e-02,\n",
            "         1.0974e-01,  4.2145e-02,  3.3484e-02, -6.5854e-02,  1.5393e-03,\n",
            "        -1.2490e-01, -9.5041e-02,  9.7780e-02, -1.0808e-01,  4.1958e-05,\n",
            "        -1.0654e-01,  5.7144e-02,  1.1191e-01, -3.4981e-02,  1.0361e-01,\n",
            "        -7.0924e-02, -7.0057e-02, -1.1717e-02,  1.7418e-02, -3.8610e-02,\n",
            "        -2.3559e-03, -2.4669e-02, -2.2082e-02, -4.4352e-02, -5.0380e-02,\n",
            "        -7.2766e-02,  3.0878e-02, -9.3229e-02, -4.8714e-02, -1.0116e-01,\n",
            "         2.0734e-02,  3.4371e-02,  7.4303e-02, -1.1427e-01,  6.8490e-02,\n",
            "         8.8468e-02, -3.9595e-02, -6.2999e-02,  1.0318e-01, -7.7768e-02,\n",
            "        -6.4843e-02, -2.3196e-02, -1.2062e-01, -1.0101e-01, -1.1501e-01,\n",
            "        -8.5373e-02, -1.3954e-02, -7.9184e-02,  9.9048e-02,  1.2504e-01,\n",
            "        -1.2164e-02, -4.6838e-02,  1.1081e-01, -3.8686e-02,  6.7181e-02,\n",
            "        -3.7828e-02, -1.5249e-01, -3.9810e-02,  6.7789e-02, -1.2977e-01,\n",
            "         2.2777e-02, -2.1639e-02,  2.1566e-01, -2.4443e-03,  2.1485e-01,\n",
            "         1.2348e-01, -8.9695e-02, -8.4034e-02,  8.3067e-03, -6.8929e-02,\n",
            "        -6.9188e-02,  8.2231e-02, -1.2967e-02, -2.7882e-02, -6.2685e-02,\n",
            "        -6.0457e-02,  4.8923e-02, -1.6019e-01, -4.7018e-02, -1.0838e-01,\n",
            "        -9.8933e-02,  3.9776e-01, -5.2971e-02,  1.4684e-01, -5.2139e-02,\n",
            "         3.2495e-02,  6.4614e-04, -4.9947e-02, -5.0213e-02, -2.4190e-01,\n",
            "         1.7382e-02,  5.9668e-03,  1.4317e-01, -7.4887e-02,  2.1185e-01,\n",
            "        -5.5125e-03, -1.6017e-01,  2.8749e-01, -2.1207e-04, -1.1156e-01,\n",
            "        -6.2216e-02,  6.9058e-03,  2.8514e-01, -6.7559e-02, -6.5080e-02,\n",
            "        -3.7988e-02,  7.2005e-02,  2.4052e-02, -1.0237e-01,  1.3738e-01,\n",
            "        -2.0119e-01, -7.0175e-02, -4.7656e-02,  1.2371e-02,  6.2557e-02,\n",
            "         5.8156e-02, -1.2363e-01, -5.7652e-02, -2.7644e-02,  3.1228e-02,\n",
            "        -2.1843e-02,  1.9661e-01,  3.4036e-01, -2.4709e-02,  7.0809e-02,\n",
            "        -2.4909e-02, -1.2739e-01,  6.9961e-03, -3.3989e-02, -1.1311e-01,\n",
            "        -1.4550e-01,  5.4258e-02, -1.3354e-01,  4.1638e-02, -9.7300e-02,\n",
            "         1.0994e-02, -1.6370e-02, -9.6261e-02, -1.0790e-01, -6.5910e-04,\n",
            "         5.8139e-02,  1.0552e-01,  3.0022e-02, -8.1520e-02, -1.5241e-01,\n",
            "         9.0653e-02, -4.4193e-02,  2.1666e-01, -5.2497e-02,  2.5458e-02,\n",
            "        -1.3685e-02, -4.1874e-02,  3.2388e-01,  1.2365e-01,  1.4967e-02,\n",
            "        -4.8856e-03,  1.1550e-01,  3.1754e-02, -2.1010e-02, -1.3373e-02,\n",
            "         1.7335e-01, -7.0874e-02,  4.0553e-01, -1.6293e-01, -8.8477e-02,\n",
            "         5.0523e-02,  2.6331e-02, -3.4983e-02,  1.0812e-02,  1.3309e-02,\n",
            "        -2.0306e-02, -2.3477e-01, -2.9526e-02, -4.6130e-02, -3.2441e-02,\n",
            "         2.5344e-02,  5.7963e-02, -3.2702e-02, -1.3632e-01,  9.3952e-02,\n",
            "        -2.3829e-01,  1.8473e-01], device='cuda:0', requires_grad=True)\n",
            "adaptiveSoftmax.head.weight Parameter containing:\n",
            "tensor([[-1.7350,  1.7943, -0.1432,  ...,  1.1322, -1.5057,  0.3411],\n",
            "        [-0.6254,  0.4260, -0.2533,  ...,  0.4528, -0.1594,  0.2374],\n",
            "        [ 0.0529, -0.0445,  0.1451,  ..., -0.0491,  0.1708,  0.1228],\n",
            "        ...,\n",
            "        [-0.0608,  0.0757, -0.0544,  ...,  0.1188, -0.1143,  0.2478],\n",
            "        [ 0.1685, -0.2742,  0.1270,  ..., -0.1294, -0.0043, -0.0676],\n",
            "        [ 0.0444, -0.1865,  0.0692,  ..., -0.0302,  0.1237, -0.0952]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "adaptiveSoftmax.tail.0.0.weight Parameter containing:\n",
            "tensor([[ 0.0767,  0.0446,  0.0361,  ..., -0.0971,  0.0303, -0.1146],\n",
            "        [ 0.0004, -0.1004,  0.1037,  ..., -0.0233,  0.0328,  0.0484],\n",
            "        [-0.0035,  0.1125, -0.0732,  ..., -0.0333, -0.0634, -0.0285],\n",
            "        ...,\n",
            "        [-0.0982,  0.0130, -0.3267,  ...,  0.0409, -0.0645, -0.2404],\n",
            "        [-0.0469, -0.0183, -0.1780,  ...,  0.0720, -0.1028, -0.0758],\n",
            "        [-0.0050, -0.0657, -0.2241,  ..., -0.0875, -0.2176, -0.1842]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "adaptiveSoftmax.tail.0.1.weight Parameter containing:\n",
            "tensor([[-0.0370, -0.3119, -0.0483,  ..., -0.1793,  0.0632, -0.2044],\n",
            "        [-0.1478,  0.1737, -0.5792,  ..., -0.2951,  0.1154, -0.1495],\n",
            "        [-0.0944, -0.1134,  0.1088,  ..., -0.0449, -0.0125, -0.0282],\n",
            "        ...,\n",
            "        [-0.3301,  0.0680, -1.0897,  ..., -0.1755,  0.6212, -0.4306],\n",
            "        [-0.2668,  0.1054, -0.4671,  ...,  0.0226,  0.4697, -0.3133],\n",
            "        [-0.8452,  0.8579, -0.8358,  ..., -0.3080,  0.7725, -0.6777]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "adaptiveSoftmax.tail.1.0.weight Parameter containing:\n",
            "tensor([[-0.0128, -0.0199, -0.0261,  ..., -0.0789, -0.0136, -0.0393],\n",
            "        [ 0.0482, -0.0352,  0.0887,  ...,  0.0086,  0.0115,  0.0670],\n",
            "        [-0.3506, -0.1046, -0.4236,  ..., -0.2007,  0.0107, -0.5591],\n",
            "        ...,\n",
            "        [ 0.0465,  0.0074, -0.0931,  ..., -0.0434, -0.2028, -0.0640],\n",
            "        [-0.0754, -0.0082, -0.0370,  ..., -0.0257,  0.0077, -0.2155],\n",
            "        [ 0.2616,  0.0142,  0.1128,  ...,  0.0692, -0.1080,  0.1649]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "adaptiveSoftmax.tail.1.1.weight Parameter containing:\n",
            "tensor([[-0.0447,  0.0759,  0.0092,  ..., -0.0467, -0.1558, -0.0213],\n",
            "        [-0.1327,  0.0978, -0.0046,  ..., -0.0217, -0.0487, -0.1391],\n",
            "        [-0.1145,  0.0237,  0.0143,  ..., -0.0009, -0.0412, -0.0586],\n",
            "        ...,\n",
            "        [-0.0822,  0.0767,  0.0162,  ...,  0.0199, -0.0573, -0.0649],\n",
            "        [-0.0817,  0.0762,  0.0168,  ...,  0.0197, -0.0587, -0.0651],\n",
            "        [-0.0819,  0.0763,  0.0169,  ...,  0.0196, -0.0589, -0.0648]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyrOXNSmvjeK"
      },
      "source": [
        "def decay_lr(optimizer, epoch, factor=0.1, lr_decay_epoch=60):\n",
        "    if epoch % lr_decay_epoch == 0:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * factor\n",
        "        print('lr decayed to %.4f' % optimizer.param_group[0]['lr'])\n",
        "    return optimizer"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W95xmnV9voFN"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9POzMH0yFd"
      },
      "source": [
        "#device = torch.device('cuda:0')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA7W5wQnvmlH"
      },
      "source": [
        "since = time.time()\n",
        "autograd.set_detect_anomaly(False)\n",
        "prev_epoch = 0 if not epoch_values else epoch_values[-1]\n",
        "best_bleu = 0 if not bleu_values else max(bleu_values)\n",
        "\n",
        "for epoch in range(num_epoch - prev_epoch):\n",
        "    epoch += prev_epoch\n",
        "    start = time.time()\n",
        "    num, total_loss = 0, 0\n",
        "#     optimizer = decay_lr(optimizer=optimizer, epoch=epoch+1)\n",
        "    topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice) # shuffle data at every epoch\n",
        "    data = data_iterator(corpus_indice, topics_indice, batch_size, max(length) + 1)\n",
        "    hidden = model.init_hidden(batch_size=batch_size)\n",
        "    weight = torch.ones(len(vocab))\n",
        "    weight[0] = 0\n",
        "    num_iter = len(corpus_indice) // batch_size\n",
        "    for X, Y, mask, topics in tqdm(data, total=num_iter):\n",
        "        num += 1\n",
        "#         hidden.detach_()\n",
        "        if use_gpu:\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            mask = mask.to(device)\n",
        "            topics = topics.to(device)\n",
        "#             hidden = hidden.to(device)\n",
        "#             hidden[0].to(device)\n",
        "#             hidden[1].to(device)\n",
        "            loss_function = loss_function.to(device)\n",
        "            weight = weight.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # init hidden layer\n",
        "#         hidden = model.init_hidden(num_layers, batch_size, hidden_dim)\n",
        "        coverage_vector = model.init_coverage_vector(batch_size, num_keywords)\n",
        "        init_output = torch.zeros(batch_size, hidden_dim).to(device)\n",
        "        # inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
        "        output, _, hidden, _, _ = model(inputs=X, topics=topics, output=init_output, hidden=hidden, mask=mask, target=Y, coverage_vector=coverage_vector)\n",
        "#         output, hidden = model(X, topics)\n",
        "        hidden[0].detach_()\n",
        "        hidden[1].detach_()\n",
        "        \n",
        "        loss = (-output.output).reshape((-1, batch_size)).t() * mask\n",
        "#         loss = loss.sum(dim=1)\n",
        "        loss = loss.sum(dim=1) / mask.sum(dim=1)\n",
        "        loss = loss.mean()\n",
        "        loss.backward()\n",
        "        \n",
        "        norm = 0.0\n",
        "#         norm = nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
        "        nn.utils.clip_grad_value_(model.parameters(), 1)\n",
        "            \n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.item())\n",
        "        \n",
        "        if np.isnan(total_loss):\n",
        "            for name, p in model.named_parameters():\n",
        "                if p.grad is None:\n",
        "                    continue \n",
        "                print(name, p)\n",
        "            assert False, \"Gradient explode\"\n",
        "    \n",
        "    one_iter_loss = np.mean(total_loss)\n",
        "    lr_scheduler.step(one_iter_loss)\n",
        "#     print(\"One iteration loss {:.3f}\".format(one_iter_loss))\n",
        "    \n",
        "    # validation\n",
        "    bleu_score = 0\n",
        "    num_test = 500\n",
        "    bleu_score = evaluate_bleu(model, topics_test, corpus_test, num_test=num_test, method='predict_rnn', is_sample=False)\n",
        "    \n",
        "    bleu_values.append(bleu_score)\n",
        "    loss_values.append(total_loss / num)\n",
        "    epoch_values.append(epoch+1)\n",
        "    \n",
        "    # save checkpoint\n",
        "    # if ((epoch + 1) % check_point == 0) or (epoch == (num_epoch - 1)) or epoch+1 > 7 or bleu_score > 4:\n",
        "    if (epoch + 1) > 0:     # save all models\n",
        "        model_check_point = '%s/model_trainable_%d.pk' % (save_folder, epoch+1)\n",
        "        optim_check_point = '%s/optim_trainable_%d.pkl' % (save_folder, epoch+1)\n",
        "        loss_check_point = '%s/loss_trainable_%d.pkl' % (save_folder, epoch+1)\n",
        "        epoch_check_point = '%s/epoch_trainable_%d.pkl' % (save_folder, epoch+1)\n",
        "        bleu_check_point = '%s/bleu_trainable_%d.pkl' % (save_folder, epoch+1)\n",
        "        torch.save(model.state_dict(), model_check_point)\n",
        "        torch.save(optimizer.state_dict(), optim_check_point)\n",
        "        torch.save(loss_values, loss_check_point)\n",
        "        torch.save(epoch_values, epoch_check_point)\n",
        "        torch.save(bleu_values, bleu_check_point)\n",
        "    \n",
        "    # save current best result\n",
        "    if bleu_score > best_bleu:\n",
        "        best_bleu = bleu_score\n",
        "        print('current best bleu: %.4f' % best_bleu)\n",
        "        model_check_point = '%s/model_best_%d.pk' % (save_folder, epoch+1)\n",
        "        optim_check_point = '%s/optim_best_%d.pkl' % (save_folder, epoch+1)\n",
        "        loss_check_point = '%s/loss_best_%d.pkl' % (save_folder, epoch+1)\n",
        "        epoch_check_point = '%s/epoch_best_%d.pkl' % (save_folder, epoch+1)\n",
        "        bleu_check_point = '%s/bleu_best_%d.pkl' % (save_folder, epoch+1)\n",
        "        torch.save(model.state_dict(), model_check_point)\n",
        "        torch.save(optimizer.state_dict(), optim_check_point)\n",
        "        torch.save(loss_values, loss_check_point)\n",
        "        torch.save(epoch_values, epoch_check_point)\n",
        "        torch.save(bleu_values, bleu_check_point)\n",
        "        \n",
        "    # calculate time\n",
        "    end = time.time()\n",
        "    s = end - since\n",
        "    h = math.floor(s / 3600)\n",
        "    m = s - h * 3600\n",
        "    m = math.floor(m / 60)\n",
        "    s -= (m * 60 + h * 3600)\n",
        "    \n",
        "\n",
        "    # verbose \n",
        "    # if ((epoch + 1) % verbose == 0) or (epoch == (num_epoch - 1)):\n",
        "    print('epoch %d/%d, loss %.4f, norm %.4f, predict bleu: %.4f, time %.3fs, since %dh %dm %ds'\n",
        "            % (epoch + 1, num_epoch, total_loss / num, norm, bleu_score, end - start, h, m, s))\n",
        "\n",
        "    evaluateAndShowAttention(['ایک', 'پاکستان', 'بھارت', 'کشمیر', 'قبضہ'], method='beam_search', is_sample=True)\n",
        "    evaluateAndShowAttention(['کرکٹ', 'بنگلہ', 'دیش', 'کھیل', 'جیت'], method='beam_search', is_sample=True)\n",
        "    evaluateAndShowAttention(['دھماکہ', 'دہشت', 'گرد', 'کیا', 'زخمی'], method='beam_search', is_sample=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o12lPtKuvufq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a3395f-81bd-42d1-ea37-7d8f5e80e2c7"
      },
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrqjXoMt0XCi"
      },
      "source": [
        "#evaluateAndShowAttention(['ایک', 'پاکستان', 'بھارت', 'کشمیر', 'قبضہ'], method='beam_search', is_sample=True)\n",
        "#evaluateAndShowAttention(['کرکٹ', 'بنگلہ', 'دیش', 'کھیل', 'جیت'], method='beam_search', is_sample=True)\n",
        "#evaluateAndShowAttention(['دھماکہ', 'دہشت', 'گرد', 'کیا', 'زخمی'], method='beam_search', is_sample=True)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkOqS3C475Vg"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hom7poD4Ky5T"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_bleu(model, topics_test, corpus_test, num_test, method='beam_search', is_sample=False):\n",
        "    num_chars = 100     # change this to set size of output\n",
        "    bleu_score = 0\n",
        "    for i in range(len(corpus_test[:num_test])):\n",
        "        if method == 'beam_search':\n",
        "            _, output_words, _, _ = beam_search([idx_to_word[x] for x in topics_test[i]], num_chars, model, idx_to_word, word_to_idx, False)\n",
        "        else:\n",
        "            _, output_words, _, _ = predict_rnn([idx_to_word[x] for x in topics_test[i]], num_chars, model, idx_to_word, word_to_idx)\n",
        "        bleu_score += sentence_bleu([[idx_to_word[x] for x in corpus_test[i] if x not in [0, 2]]], output_words, weights=(1, 0, 0, 0))\n",
        "        \n",
        "    bleu_score = bleu_score / num_test\n",
        "    return bleu_score"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jBuSbMSOtTn"
      },
      "source": [
        "AllBleu = []\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSie3ZJHdN5B"
      },
      "source": [
        "import random"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCUOG2BKmGY"
      },
      "source": [
        "AllBleu = {}\n",
        "for i in range(1,80):\n",
        "  version_num = i\n",
        "  Type = 'trainable'\n",
        "  model_check_point = '%s/model_%s_%d.pk' % (save_folder, Type, version_num)\n",
        "  optim_check_point = '%s/optim_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "  loss_check_point = '%s/loss_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "  epoch_check_point = '%s/epoch_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "  bleu_check_point = '%s/bleu_%s_%d.pkl' % (save_folder, Type, version_num)\n",
        "  loss_values = []\n",
        "  epoch_values = []\n",
        "  bleu_values = []\n",
        "  if os.path.isfile(model_check_point):\n",
        "      print('Loading previous status (ver.%d)...' % version_num)\n",
        "      model.load_state_dict(torch.load(model_check_point, map_location='cpu'))\n",
        "      model = model.to(device)\n",
        "      optimizer.load_state_dict(torch.load(optim_check_point))\n",
        "      lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
        "      loss_values = torch.load(loss_check_point)\n",
        "      epoch_values = torch.load(epoch_check_point)\n",
        "      bleu_values = torch.load(bleu_check_point)\n",
        "      print('Load successfully')\n",
        "      score = evaluate_bleu(model, topics_test, corpus_test, num_test=100, method='predict_rnn', is_sample=False)\n",
        "\n",
        "      print(\"BleuScore:\",score)\n",
        "\n",
        "      AllBleu[i]=score\n",
        "\n",
        "    \n",
        "  else:\n",
        "      print(\"ver.%d doesn't exist\" % version_num)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0VFeOfBMSXK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "# x axis values\n",
        "x = AllBleu.keys()\n",
        "# corresponding y axis values\n",
        "y = AllBleu.values()\n",
        "  \n",
        "# plotting the points \n",
        "plt.plot([0]+list(x), [0]+list(y))\n",
        "  \n",
        "# naming the x axis\n",
        "plt.xlabel('no.of epochs trained')\n",
        "# naming the y axis\n",
        "plt.ylabel('Bleu score')\n",
        "  \n",
        "# giving a title to my graph\n",
        "plt.title('BLEU Scores for different epochs - Google Colab')\n",
        "  \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRv3hYi-qeOD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0m-NtV7M8Lp"
      },
      "source": [
        "def inference(input_sentence, method='beam_search', is_sample=False, showResult=False):\n",
        "    num_chars = 100     # change this to set size of output\n",
        "    if method == 'beam_search':\n",
        "        _, output_words, attentions, coverage_vector = beam_search(input_sentence, num_chars, model, idx_to_word, word_to_idx, is_sample=is_sample)\n",
        "    else:\n",
        "        _, output_words, attentions, _ = predict_rnn(input_sentence, num_chars, model, idx_to_word, word_to_idx)\n",
        "\n",
        "    if showResult:\n",
        "      print('input =', ' '.join(input_sentence))\n",
        "      print('output =', ' '.join(output_words))\n",
        "    return output_words[1:]\n",
        "\n",
        "inference([idx_to_word[word] for word in topics_indice[100]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHrFFZw_O1mR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}